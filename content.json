{"posts":[{"title":"Between Sampling and Optimization","text":"Introduction​ Let $p$ be a positive integer and $f:\\mathbb{R}^p\\to \\mathbb{R}$ be a measurable function such that$$\\int_{\\mathbb{R}^p} e^{-f(x)} dx&lt;+\\infty.$$In various applications, one is faced with the problem of finding the minimum point of $f$ or computing the average with respect to the probability density$$\\pi(x) = \\dfrac{e^{-f(x)}}{\\int_{\\mathbb{R}^p} e^{-f(u)}du}.$$ In other words, one often looks for approximating the values $ x^{*} $ and $ \\bar x $ defined as $$x^{*} \\in \\arg\\min_{x\\in \\mathbb{R}^p} f(x), \\quad \\bar x = \\int_{\\mathbb{R}^p} x \\pi(x) dx.$$ AssumptionAssumption 1. We assume $f$ is strongly convex with Lipschitz continuous gradient. That is, we assume that there exist two positive constants $m$ and $M$ such that for all $x,y\\in \\mathbb{R}^p$,$$f(y)-f(x)-\\nabla f(x)^T (y-x)\\ge \\frac{m}{2}||y-x||_2^2,$$and$$||\\nabla f(x)-\\nabla f(y)||_2\\le M ||x-y||_2.$$ Langevin Monte Carlo (LMC) Algorithm​ Starting from an initial point $x^{(0)}\\in \\mathbb{R}^p$ that maybe deterministic or random, the iterations of the algorithm are defined by the update rule$$x^{(k+1,h)} = x^{(k,h)}-h\\nabla f(x^{(k,h)})+\\sqrt{2h}\\ \\xi^{(k+1)};\\quad k=0,1,2,\\cdots \\tag{LMC}$$where $h&gt;0$ is the tuning parameter, refer to as the step size, and $\\xi^{(1)},\\cdots,\\xi^{(k)},\\cdots$ is a sequence of mutually independent, and independent of $x^{(0)}$, centered Gaussian vectors with covariance matrices equal to identity. Let $\\nu_k$ be the distribution of the $k$-th iterate of the LMC algorithm, that is $x^{(k,h)}\\sim\\nu_k$. Main Idea of LMC Algorithm​ The Markov chain $\\{x^{(k,h)}\\}_{k\\in \\mathbb{N}}$ is the Euler-Maruyama discretization of the following Langevin dynamic SDE:$$dL_t = -\\nabla f(L_t)+\\sqrt{2}dW_t,\\quad t\\ge 0,$$where $\\{W_t;\\ t\\ge 0\\}$ is a $p-$dimensional Brownian motion, $\\{L_t; t\\ge 0\\}$ is known as Langevin diffusion. When $f$ satisfies Assumption 1, the above SDE has a unique strong solution which is a Markov process. ​ By Fokker-Planck Equation, we know $\\{L_t; t\\ge 0\\}$ has $\\pi$ as invariant density, namely if $L_0\\sim \\pi$ then $L_t\\sim \\pi$ for all $t\\ge 0$. ​ Under Assumption 1, when $h$ is small and $k$ is large (so that the product $kh$ is large), the distribution of $x^{(k,h)}$ is close in various metrics to the distribution $\\pi$ with density $\\pi(x)$. So the question is how to quantify this closeness? Wasserstein DistanceDefinition 1. For two measures $\\mu$ and $\\nu$ defined on $(\\mathbb{R}^p,\\mathcal{B}(\\mathbb{R}^p))$, the $2-$Wasserstein distance is defined by$$W_2(\\mu,\\nu) = \\left( \\inf_{\\gamma\\in \\Gamma(\\mu,\\nu)}\\int_{\\mathbb{R}^p\\times \\mathbb{R}^p}||x-y||_2^2d\\gamma(x,y)\\right)^{1/2},$$where the infimum is with respect to all joint distributions $\\gamma$ having $\\mu$ and $\\nu$ as marginal distributions. Here we review some important properties of Wasserstein distance. Proposition 1. For two Dirac measure $\\delta_x$ and $\\delta_y$, we have $$W_2(\\delta_x,\\delta_y) = ||x-y||_2.$$Proposition 2. The infimum in the definition of Wasserstein distance is achievable. That is to say there exists a joint distribution $\\gamma^*\\in \\Gamma(\\mu,\\nu)$ such that $$W_2(\\mu,\\nu) = \\left( \\int_{\\mathbb{R}^p\\times \\mathbb{R}^p}||x-y||_2^2d\\gamma^*(x,y)\\right)^{1/2}.$$ Main Theorem Theorem 1. Assume that $h\\in (0,\\frac{2}{M})$. The following claims hold: (a) If $h\\le \\frac{2}{m+M}$ then $W_2(\\nu_k,\\pi)\\le (1-mh)^k W_2(\\nu_0, \\pi)+ \\frac{M}{m}(5hp/3)^{1/2}.$ (b) If $h\\ge \\frac{2}{m+M}$ then $W_2(\\nu_k,\\pi)\\le (Mh-1)^k W_2(\\nu_0, \\pi)+ \\frac{Mh}{2-Mh}(5hp/3)^{1/2}.$ Relation with OptimizationThe function $f(x)$ and the function $f_\\tau(x):=f(x)/\\tau$ have the same minimizer $x^{*}$, whatever the real number $\\tau&gt;0$. If we define the density function $\\pi_\\tau(x)\\propto \\exp(-f_\\tau(x))$, then the average value$$\\bar x_\\tau = \\int_{\\mathbb{R}^p} x \\pi_\\tau (x)dx$$tends to the minimizer $x^* $ when $\\tau\\to 0$. Furthermore, the distribution $\\pi_{\\tau}$ tends to the Dirac measure $\\delta_{x^{*}}$.","link":"/2025/09/16/LMC_GD/"},{"title":"Introduction to Flow Matching","text":"BackgroundFor a particle $x_0\\in \\mathbb{R}^d$, given Lipschitz continuous velocity field $\\{v_t\\}_{t\\in [0,1]}$, there is a unique trajectory of $x_0$ described by$$\\begin{cases}\\begin{split}\\dfrac{d x_t}{dt} &amp;= v_t(x_t),\\quad t\\in [0,1]\\x_0&amp;=x_0\\end{split}\\end{cases}.\\tag{Flow-ODE}$$Flow $\\Phi$ gives the collection of trajectories with different initial positions $x_0$, defined by$$\\begin{split}\\Phi: [0,1]\\times \\mathbb{R}^d&amp;\\to \\mathbb{R}^d\\(t,x_0)&amp;\\mapsto \\Phi(t,x_0)=x_t,\\end{split}$$where $x_t$ denotes the solution of (Flow-ODE) at time $t$. If we have already known the flow mapping $\\Phi$, then for all initial position $x_0\\in\\mathbb{R}^d$, we can get the terminal position $x_1$ by $\\Phi(1,x_0)$. To get the flow mapping $\\Phi$, the only thing we need is the vector field $\\{v_t\\}_{t\\in [0,1]}$ and solve the ordinary differential equation (Flow-ODE).","link":"/2025/11/26/flow_matching/"},{"title":"Hello Everyone","text":"Hello everyone! Welcome to my blog! I’ll be sharing my journey as a master student at the Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences. This blog isn’t just about my math research – I’ll also explore topics like economics, finance, and anything else that sparks my interest! If you have any questions or just want to reach out, feel free to email me at wangtuo1020@outlook.com.","link":"/2025/09/11/hello/"}],"tags":[{"name":"Sampling","slug":"Sampling","link":"/tags/Sampling/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Paper Reading","slug":"Paper-Reading","link":"/tags/Paper-Reading/"},{"name":"Generative Modeling","slug":"Generative-Modeling","link":"/tags/Generative-Modeling/"}],"categories":[{"name":"Sampling","slug":"Sampling","link":"/categories/Sampling/"}],"pages":[{"title":"About Me","text":"Hello","link":"/about.html"},{"title":"What is Economy?","text":"Three Main Problems for Economy How People Make Decisions How People Interact How the Economy as a Whole Works Ten Principles of EconomyHow People Make Decisions People face tradeoffs. The cost of something is what you give up to get it. Rational people think at the margin. People respond to incentives. How People Interact Trade can make everyone better off. Markets are usually a good way to organize economic activity. Governments can sometimes improve market outcomes. How the Economy as a Whole Works The standard of living depends on a country’s production. Prices rise when the government prints too much money. Society faces a short-run tradeoff between inflation and unempolyment. 1. People face tradeoffsThere is no such thing as a free lunch! To get one thing, we usually have to give up another thing. 2. The cost of something is what you give up to get itThe opportunity cost of an item is what you give up to obtain that item. 3. Rational people think at the marginMarginal changes are small, incremental adjustments to an existing plan of action. 4. People respond to incentivesMarginal changes in costs or benefits motivate people to respond. The decision to choose one alternative over another occurs when that alternative’s marginal benefits exceed its marginal costs! 5. Trade can make everyone better offPeople gain from their ability to trade with one another. Competition results in gains from trading. Trade allows people to specialize in what they do best.","link":"/economy_what_is_economy.html"},{"title":"About Me","text":"Hello","link":"/about/about.html"},{"title":"An Introduction to Sampling","text":"Sampling algorithms provide efficient ways to generate samples from complex probability distributions. They are widely applied in areas such as Bayesian inference, optimization, physics, and machine learning, making them essential tools for connecting theory with practical data analysis. The Goal of SamplingThe goal of sampling is to generate random variable $X\\sim \\pi$ from a target distribution $\\pi$, typically known up to a normalization constant$$\\pi \\propto e^{-V(x)}$$","link":"/sampling_introduction.html"}]}