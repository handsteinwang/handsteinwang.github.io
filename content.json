{"posts":[{"title":"Between Sampling and Optimization","text":"Introduction​ Let $p$ be a positive integer and $f:\\mathbb{R}^p\\to \\mathbb{R}$ be a measurable function such that$$\\int_{\\mathbb{R}^p} e^{-f(x)} dx&lt;+\\infty.$$In various applications, one is faced with the problem of finding the minimum point of $f$ or computing the average with respect to the probability density$$\\pi(x) = \\dfrac{e^{-f(x)}}{\\int_{\\mathbb{R}^p} e^{-f(u)}du}.$$ In other words, one often looks for approximating the values $ x^{*} $ and $ \\bar x $ defined as $$x^{*} \\in \\arg\\min_{x\\in \\mathbb{R}^p} f(x), \\quad \\bar x = \\int_{\\mathbb{R}^p} x \\pi(x) dx.$$ AssumptionAssumption 1. We assume $f$ is strongly convex with Lipschitz continuous gradient. That is, we assume that there exist two positive constants $m$ and $M$ such that for all $x,y\\in \\mathbb{R}^p$,$$f(y)-f(x)-\\nabla f(x)^T (y-x)\\ge \\frac{m}{2}||y-x||_2^2,$$and$$||\\nabla f(x)-\\nabla f(y)||_2\\le M ||x-y||_2.$$ Langevin Monte Carlo (LMC) Algorithm​ Starting from an initial point $x^{(0)}\\in \\mathbb{R}^p$ that maybe deterministic or random, the iterations of the algorithm are defined by the update rule$$x^{(k+1,h)} = x^{(k,h)}-h\\nabla f(x^{(k,h)})+\\sqrt{2h}\\ \\xi^{(k+1)};\\quad k=0,1,2,\\cdots \\tag{LMC}$$where $h&gt;0$ is the tuning parameter, refer to as the step size, and $\\xi^{(1)},\\cdots,\\xi^{(k)},\\cdots$ is a sequence of mutually independent, and independent of $x^{(0)}$, centered Gaussian vectors with covariance matrices equal to identity. Let $\\nu_k$ be the distribution of the $k$-th iterate of the LMC algorithm, that is $x^{(k,h)}\\sim\\nu_k$. Main Idea of LMC Algorithm​ The Markov chain $\\{x^{(k,h)}\\}_{k\\in \\mathbb{N}}$ is the Euler-Maruyama discretization of the following Langevin dynamic SDE:$$dL_t = -\\nabla f(L_t)+\\sqrt{2}dW_t,\\quad t\\ge 0,$$where $\\{W_t;\\ t\\ge 0\\}$ is a $p-$dimensional Brownian motion, $\\{L_t; t\\ge 0\\}$ is known as Langevin diffusion. When $f$ satisfies Assumption 1, the above SDE has a unique strong solution which is a Markov process. ​ By Fokker-Planck Equation, we know $\\{L_t; t\\ge 0\\}$ has $\\pi$ as invariant density, namely if $L_0\\sim \\pi$ then $L_t\\sim \\pi$ for all $t\\ge 0$. ​ Under Assumption 1, when $h$ is small and $k$ is large (so that the product $kh$ is large), the distribution of $x^{(k,h)}$ is close in various metrics to the distribution $\\pi$ with density $\\pi(x)$. So the question is how to quantify this closeness? Wasserstein DistanceDefinition 1. For two measures $\\mu$ and $\\nu$ defined on $(\\mathbb{R}^p,\\mathcal{B}(\\mathbb{R}^p))$, the $2-$Wasserstein distance is defined by$$W_2(\\mu,\\nu) = \\left( \\inf_{\\gamma\\in \\Gamma(\\mu,\\nu)}\\int_{\\mathbb{R}^p\\times \\mathbb{R}^p}||x-y||_2^2d\\gamma(x,y)\\right)^{1/2},$$where the infimum is with respect to all joint distributions $\\gamma$ having $\\mu$ and $\\nu$ as marginal distributions. Here we review some important properties of Wasserstein distance. Proposition 1. For two Dirac measure $\\delta_x$ and $\\delta_y$, we have $$W_2(\\delta_x,\\delta_y) = ||x-y||_2.$$Proposition 2. The infimum in the definition of Wasserstein distance is achievable. That is to say there exists a joint distribution $\\gamma^*\\in \\Gamma(\\mu,\\nu)$ such that $$W_2(\\mu,\\nu) = \\left( \\int_{\\mathbb{R}^p\\times \\mathbb{R}^p}||x-y||_2^2d\\gamma^*(x,y)\\right)^{1/2}.$$ Main Theorem Theorem 1. Assume that $h\\in (0,\\frac{2}{M})$. The following claims hold: (a) If $h\\le \\frac{2}{m+M}$ then $W_2(\\nu_k,\\pi)\\le (1-mh)^k W_2(\\nu_0, \\pi)+ \\frac{M}{m}(5hp/3)^{1/2}.$ (b) If $h\\ge \\frac{2}{m+M}$ then $W_2(\\nu_k,\\pi)\\le (Mh-1)^k W_2(\\nu_0, \\pi)+ \\frac{Mh}{2-Mh}(5hp/3)^{1/2}.$ Relation with OptimizationThe function $f(x)$ and the function $f_\\tau(x):=f(x)/\\tau$ have the same minimizer $x^{*}$, whatever the real number $\\tau&gt;0$. If we define the density function $\\pi_\\tau(x)\\propto \\exp(-f_\\tau(x))$, then the average value$$\\bar x_\\tau = \\int_{\\mathbb{R}^p} x \\pi_\\tau (x)dx$$tends to the minimizer $x^* $ when $\\tau\\to 0$. Furthermore, the distribution $\\pi_{\\tau}$ tends to the Dirac measure $\\delta_{x^{*}}$.","link":"/2025/09/16/LMC_GD/"},{"title":"Introduction to Flow Matching","text":"In generative modeling, we are given a collection of training samples $\\{x_i\\}_{i=1}^N$ and wish to generate new samples from the underlying target distribution $\\pi$. There are already many established approaches to this problem, including likelihood-based methods, implicit generative models such as GANs, and score-based diffusion models. More recently, the flow matching framework has emerged as another powerful paradigm. In what follows, we introduce the basic ideas of flow matching and explain how works. BackgroundFor a particle initially at position $x_0 \\in \\mathbb{R}^d$ and a (Lipschitz continuous) velocity field $\\{v_t\\}$, there exists a unique trajectory $\\{x_t\\}$ described by $$\\begin{cases}\\begin{split}\\dfrac{d x_t}{dt} &amp;= v_t(x_t),\\quad t\\in [0,1]\\\\\\x_0&amp;=x_0\\end{split}\\end{cases}.\\tag{Flow-ODE}$$The flow $\\Phi$ collects the trajectories corresponding to different initial positions $x_0$ and is defined by $$\\begin{split}\\Phi: [0,1]\\times \\mathbb{R}^d&amp;\\to \\mathbb{R}^d\\\\\\(t,x_0)&amp;\\mapsto \\Phi(t,x_0)=x_t,\\end{split}$$where $x_t$ denotes the solution of (Flow-ODE) at time $t$. If the flow mapping $\\Phi$ is known, then for any initial position $x_0 \\in \\mathbb{R}^d$ we can obtain the terminal position $x_1$ via $x_1 = \\Phi(1,x_0)$. To determine the flow mapping $\\Phi$, it suffices to know the velocity field $\\{v_t\\}_{t \\in [0,1]}$ and to solve the ordinary differential equation (Flow-ODE). Figure 1: A particle moves from $x_0$ to $x_1$ along the velocity field $v_t$. In general, we aim to use the flow mapping $\\Phi$ to transport the initial distribution $\\mu_0$ to the target distribution $\\pi$. Figure 2: Transport the initial distribution $\\mu_0$ to the target distribution $\\pi$ by the flow mapping $\\Phi$ (or the corresponding vector field $v_t$). Let $\\{X_t\\}$ be a stochastic process with $\\operatorname{Law}(X_t)=\\mu_t$ such that$$\\begin{cases}\\begin{split}\\dfrac{d X_t}{dt} &amp;= v_t(X_t),\\ a.e. \\\\\\X_0&amp;\\sim \\mu_0\\end{split}\\end{cases},\\quad t\\in [0,1]$$Then we know $(\\mu_t,v_t)$ must satisfies the following continuity equation$$\\partial_t \\mu_t +\\nabla\\cdot (\\mu_tv_t) = 0,$$which means for all $\\varphi\\in C_c^{\\infty}((0,1)\\times \\mathbb{R}^d)$, we have$$\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t(dy)dt=0.\\tag{CE}$$If $\\mu_t$ has density $\\rho_t$ with respect to Lebesgue measure on $\\mathbb{R}^d$, continuity equation can be written as$$\\partial_t \\rho_t +\\nabla\\cdot (\\rho_tv_t) = 0.$$ Note: The ODE formulation$$\\begin{cases}\\begin{split}\\dfrac{d X_t}{dt} &amp;= v_t(X_t),\\ \\text{a.e.} \\\\\\X_0 &amp;\\sim \\mu_0\\end{split}\\end{cases}\\qquad t\\in [0,1]$$is the Lagrangian (particle) viewpoint, while the continuity equation$$\\partial_t \\mu_t + \\nabla\\cdot(\\mu_t v_t) = 0$$is the Eulerian (distribution) viewpoint. One can show that the Lagrangian formulation implies the Eulerian one. Conversely, under suitable assumptions, if the continuity equation holds, then there exists a stochastic process $(X_t)_{t\\in[0,1]}$ solving the ODE above. This result is often referred to as the superposition principle (see [1] for a reference). Hence, once we have learned a “good” flow map $\\Phi$, we can sample $X_0$ from the initial distribution $\\mu_0$ (for instance, a Gaussian $N(0, I_d)$), and then obtain$$\\operatorname{Law}(X_1) =\\operatorname{Law}(\\Phi(1, X_0)) =\\mu_1 \\approx \\pi.$$The central question is therefore: how can we learn such a “good” flow map $\\Phi$, or equivalently, how can we design the vector field $\\{v_t\\}$ using only the training data $\\{x_i\\}_{i=1}^N \\stackrel{\\text{i.i.d.}}{\\sim} \\pi$? Flow MatchingThe main idea of flow matching is to approximate the target vector field $v_t$ by a neural network $u_t^{(\\theta)}$. The neural network is trained by minimizing a suitable discrepancy between $u_t^{(\\theta)}$ and an explicitly known conditional vector field $v_t^x$. To make this precise, we first introduce the conditional probability paths $\\mu_t^x$ and the corresponding conditional vector fields $v_t^x$, and then explain how they relate to the marginal probability path $\\mu_t$ and the marginal vector field $v_t$ that we ultimately wish to learn. Conditional Probability Path and Conditional Vector FieldDefinition 1. The conditional probability path is a path of Markov kernel denoted by $\\mu_t^x$ satisfying: (1) For all $t\\in [0,1]$ and $A\\in \\mathcal{B}(\\mathbb{R}^d)$, the mapping $x\\mapsto \\mu_t^x(A)$ is $\\mathcal{B}(\\mathbb{R}^d)$-measurable; (2) For all $t\\in [0,1]$ and $x\\in \\mathbb{R}^d$, $\\mu_t^x(\\cdot)$ is a probability measure on $\\mathbb{R}^d$; (3) For all $x\\in \\mathbb{R}^d$, $\\mu_0^x\\equiv \\mu_0$ and $\\mu_1^x=\\delta_x$. Note: Property (3) above is used to ensure that the marginal probability path$$\\mu_t(A) := \\int_{\\mathbb{R}^d} \\mu_t^x(A)\\ \\pi(dx), \\quad \\forall A \\in \\mathcal{B}(\\mathbb{R}^d),$$satisfies $\\mu_0 = \\mu_0$ (our prescribed initial distribution) and$$\\mu_1(A) = \\int_{\\mathbb{R}^d} \\delta_x(A)\\ \\pi(dx) = \\int_A \\pi(dx) = \\pi(A), \\quad \\forall A \\in \\mathcal{B}(\\mathbb{R}^d),$$which implies that $\\mu_1 = \\pi$. Hence $\\mu_t$ is a probability path interpolating between the initial distribution $\\mu_0$ and the target distribution $\\pi$, which is exactly what we want. In practice, one may choose a sufficiently small $\\sigma_{\\min} &gt; 0$ and set $\\mu_1^x = N(x, \\sigma_{\\min}^2)$, so that in the end $\\mu_1 \\approx \\pi$. Construction of Conditional Probability PathThere are many ways to construct a conditional probability path $\\mu_t^x$ satisfying the conditions in Definition 1. Here, we adopt a simple conditional Gaussian path$$\\mu_t^x = N(m_t^x,(\\sigma_t^x)^2I_d),$$where$$m_t^x = tx\\quad \\text{and}\\quad (\\sigma_t^x)^2 = (1-t)^2\\sigma_0^2+\\sigma_{\\min}^2,$$with $\\sigma_0^2=1-\\sigma_{\\min}^2$. It is easy to see that$$\\mu_0^x = N(0,I_d)\\quad \\text{and}\\quad \\mu_1^x = N(x,\\sigma_{\\min}^2I_d)\\approx \\delta_x.$$Naturally, we hope our trajectory is$$Y_t^x = m_t^x+\\sigma_t^x Z,$$where $Z\\sim N(0,I_d)$ and we have $\\operatorname{Law}(Y_t^x)=\\mu_t^x$. By differentiating $Y_t^x$ with respect to $t$, we obtain$$\\begin{aligned}\\dfrac{d}{dt}Y_t^x &amp;= \\dot m_t^x+\\dot \\sigma_t^x Z\\\\\\&amp;=\\dot m_t^x+\\dfrac{\\dot \\sigma_t^x}{\\sigma_t^x} (Y_t^x-m_t^x),\\end{aligned}$$where $\\dot m_t^x = \\dfrac{d}{dt} m_t^x$ and $\\dot \\sigma_t^x = \\dfrac{d}{dt} \\sigma_t^x$. Hence, we can choose the conditional vector field$$v_t^x(y):= \\dot m_t^x+\\dfrac{\\dot \\sigma_t^x}{\\sigma_t^x} (y-m_t^x).$$Then we have$$\\begin{cases}\\dfrac{d}{dt}Y_t^x = v_t^x(Y_t^x),\\ \\operatorname{Law}(Y_t^x)=\\mu_t^x\\\\\\Y_0^x\\sim \\mu_0^x\\end{cases},\\quad t\\in [0,1].$$Therefore, $(\\mu_t^x, v_t^x)$ satisfies the continuity equation: for all $\\varphi\\in C_c^{\\infty}((0,1)\\times \\mathbb{R}^d)$, we have$$\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t^x(dy)dt=0.\\tag{CE-conditional}$$ Summary We construct the conditional probability path$$\\mu_t^x = N(m_t^x,(\\sigma_t^x)^2I_d),$$where$$m_t^x = tx\\quad \\text{and}\\quad (\\sigma_t^x)^2 = (1-t)^2\\sigma_0^2+\\sigma_{\\min}^2,$$with $\\sigma_0^2=1-\\sigma_{\\min}^2$ and it satisfies$$\\mu_0^x = N(0,I_d)\\quad \\text{and}\\quad \\mu_1^x = N(x,\\sigma_{\\min}^2I_d)\\approx \\delta_x.$$Then we construct the conditional vector field$$v_t^x(y):= \\dot m_t^x+\\dfrac{\\dot \\sigma_t^x}{\\sigma_t^x} (y-m_t^x).$$And then $(\\mu_t^x, v_t^x)$ satisfies the continuity equation: for all $\\varphi\\in C_c^{\\infty}((0,1)\\times \\mathbb{R}^d)$, we have$$\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t^x(dy)dt=0.\\tag{CE-conditional}$$ Marginal Probability Path and Marginal Vector FieldWe define the marginal probability path $\\mu_t$ by$$\\mu_t(A) := \\int_{\\mathbb{R}^d} \\mu_t^x(A)\\ \\pi(dx), \\quad \\forall A \\in \\mathcal{B}(\\mathbb{R}^d).$$We have $\\mu_t$ start at our initial distribution $\\mu_0$ and end by$$\\mu_1\\approx \\pi.$$What’s more, we have the following lemma. Lemma 1. For $\\pi-$a.e. $x$, we have $\\mu_t^x\\ll \\mu_t$. Proof : If $\\mu_t(A)=0$, then$$\\int_{\\mathbb{R}^d} \\mu_t^x(A)\\ \\pi(dx)=0.$$Since $\\mu_t^x(A)\\ge 0$, we have for $\\pi-$a.e. $x$, $\\mu_t^x(A)=0$. Hence, for $\\pi-$a.e. $x$, we have $\\mu_t^x\\ll \\mu_t$. By Lemma 1, for $\\pi-$a.e. $x$, the Radon-Nikodym derivative$$\\dfrac{d\\mu_t^x}{d\\mu_t}$$exists. Then we can define the marginal vector field as$$v_t(y):= \\int_{\\mathbb{R}^d} v_t^x(y) \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\pi(dx).$$Note: To some extent, $v_t$ is a weighted average of $v_t^x$. Theorem 2. $(\\mu_t, v_t)$ satisfies the continuity equation. Proof : For all $\\varphi\\in C_c^{\\infty}((0,1)\\times \\mathbb{R}^d)$, we need to show that$$I=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t(dy)dt=0.$$In fact, $I=I_1+I_2$, where$$\\begin{aligned}I_1&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\dfrac{\\partial}{\\partial t}\\varphi(t,y)\\mu_t(dy)dt\\\\\\I_2&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t(y), \\nabla\\varphi(t,y)\\rangle\\mu_t(dy)dt.\\end{aligned}$$By using$$\\mu_t(dy) =\\int_{\\mathbb{R}^d} \\mu_t^x(dy)\\ \\pi(dx)$$and$$v_t(y):= \\int_{\\mathbb{R}^d} v_t^x(y) \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\pi(dx),$$we have$$\\begin{aligned}I_1&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\dfrac{\\partial}{\\partial t}\\varphi(t,y)\\ \\mu_t(dy)\\ dt\\\\\\&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\dfrac{\\partial}{\\partial t}\\varphi(t,y)\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt\\\\\\&amp;\\xlongequal{\\text{Fubini}}\\int_{\\mathbb{R}^d}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\dfrac{\\partial}{\\partial t}\\varphi(t,y)\\ \\mu_t^x(dy)\\ dt\\ \\pi(dx)\\end{aligned}$$and$$\\begin{aligned}I_2&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t(y), \\nabla\\varphi(t,y)\\rangle\\ \\mu_t(dy)\\ dt\\\\\\&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\ \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\pi(dx)\\ \\mu_t(dy)\\ dt\\\\\\&amp;\\xlongequal{\\text{Fubini}}\\int_{\\mathbb{R}^d}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\ \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\mu_t(dy)\\ dt \\ \\pi(dx)\\\\\\&amp;\\xlongequal{\\text{Chain Rule}}\\int_{\\mathbb{R}^d}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\ \\mu_t^x(dy)\\ dt \\ \\pi(dx).\\end{aligned}$$Hence by $(\\mu_t^x, v_t^x)$ satisfies the continuity equation, namely$$\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t^x(dy)dt=0$$we get$$I=I_1+I_2 =\\int_{\\mathbb{R}^d}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\right]\\ \\mu_t^x(dy)\\ dt\\ \\pi(dx)=0,$$which means $(\\mu_t, v_t)$ satisfies the continuity equation. By Theorem 2, there exists a stochastic process $X_t$ with $\\operatorname{Law}(X_t)=\\mu_t$ such that$$\\begin{cases}\\begin{split}\\dfrac{d X_t}{dt} &amp;= v_t(X_t),\\ a.e. \\\\\\X_0&amp;\\sim \\mu_0\\end{split}\\end{cases},\\quad t\\in [0,1]$$And $\\mu_t$ start at our initial distribution $\\mu_0$ and end by$$\\mu_1\\approx \\pi.$$ Summary We construct the marginal probability path and marginal vector field by$$\\begin{aligned}\\mu_t(A) &amp;:= \\int_{\\mathbb{R}^d} \\mu_t^x(A)\\ \\pi(dx), \\quad \\forall A \\in \\mathcal{B}(\\mathbb{R}^d).\\\\\\v_t(y)&amp;:= \\int_{\\mathbb{R}^d} v_t^x(y) \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\pi(dx).\\end{aligned}$$We have $(\\mu_t, v_t)$ satisfies the continuity equation, which means there exists a stochastic process $X_t$ with $\\operatorname{Law}(X_t)=\\mu_t$ such that$$\\begin{cases}\\begin{split}\\dfrac{d X_t}{dt} &amp;= v_t(X_t),\\ a.e. \\\\\\X_0&amp;\\sim \\mu_0\\end{split}\\end{cases},\\quad t\\in [0,1].$$And $\\mu_t$ start at our initial distribution $\\mu_0$ and end by$$\\mu_1\\approx \\pi.$$ Now we can answer the question about how to train the neural network $u_t^{(\\theta)}$. TrainingTheorem 3. Define the marginal loss $L(\\theta)$ and conditional loss $J(\\theta)$ by$$\\begin{aligned}L(\\theta)&amp;:= \\int_{0}^{1}\\int_{\\mathbb{R}^d}|v_t(y)-u_t^{(\\theta)}(y)|^2\\ \\mu_t(dy)\\ dt\\\\\\J(\\theta)&amp;:= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}|v_t^x(y)-u_t^{(\\theta)}(y)|^2\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt\\end{aligned}$$We have$$L(\\theta) = J(\\theta)+C,$$where $C$ is a constant independent of $\\theta$. Hence,$$\\arg\\min_{\\theta} L(\\theta)=\\arg\\min_{\\theta} J(\\theta).$$Proof : Expand $L(\\theta)$ and $J(\\theta)$ as$$\\begin{aligned}L(\\theta)&amp;= \\underbrace{\\int_{0}^{1}\\int_{\\mathbb{R}^d}|v_t(y)|^2\\ \\mu_t(dy)\\ dt}{A_1}-2\\underbrace{\\int{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t(y), u_t^{(\\theta)}(y)\\rangle\\ \\mu_t(dy)\\ dt}{B_1}+\\underbrace{\\int{0}^{1}\\int_{\\mathbb{R}^d}|u_t^{(\\theta)}|^2\\ \\mu_t(dy)\\ dt}{C_1}\\\\\\J(\\theta)&amp;= \\underbrace{\\int{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}|v_t^x(y)|^2\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt}{A_2}-2\\underbrace{\\int{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), u_t^{(\\theta)}(y)\\rangle\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt}{B_2}+\\underbrace{\\int{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}|u_t^{(\\theta)}|^2\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt}{C_2}\\end{aligned}$$By using$$\\mu_t(dy) =\\int{\\mathbb{R}^d} \\mu_t^x(dy)\\ \\pi(dx)$$and$$v_t(y):= \\int_{\\mathbb{R}^d} v_t^x(y) \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\pi(dx),$$we have$$\\begin{aligned}C_1 &amp;= \\int_{0}^{1}\\int_{\\mathbb{R}^d}|u_t^{(\\theta)}|^2\\ \\mu_t(dy)\\ dt\\\\\\&amp;= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}|u_t^{(\\theta)}|^2\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt\\\\\\&amp;=C_2\\end{aligned}$$and$$\\begin{aligned}B_1&amp;= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t(y), u_t^{(\\theta)}(y)\\rangle\\ \\mu_t(dy)\\ dt\\\\\\&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), u_t^{(\\theta)}(y)\\rangle\\ \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\pi(dx)\\ \\mu_t(dy)\\ dt\\\\\\&amp;\\xlongequal{\\text{Fubini}}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), u_t^{(\\theta)}(y)\\rangle\\ \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\mu_t(dy)\\ \\pi(dx)\\ dt\\\\\\&amp;\\xlongequal{\\text{Chain Rule}} \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), u_t^{(\\theta)}(y)\\rangle\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt\\\\\\&amp;=B_2.\\end{aligned}$$Since $A_1$ and $A_2$ are independent of $\\theta$, the desired result follows directly. By Theorem 3, we can sample $\\{t_k\\}_{k=1}^K\\sim U[0,1]$; draw from training data $\\{x_i\\}_{i=1}^N$; for all $(t_k, x_i)$, draw $M$ samples $\\{y_m^{(t_k,x_i)}\\}_{m=1}^M\\sim \\mu_t^x=N(m_t^x,(\\sigma_t^x)^xI_d)$. And then train our neural network by minimizing$$\\theta^*\\in \\arg\\min_\\theta\\ \\dfrac{1}{KNM}\\sum_{k=1}^K\\sum_{i=1}^N\\sum_{m=1}^M |v_{t_k}^{x_i}(y_m^{(t_k,x_i)})-u_{t_k}^{(\\theta)}(y_m^{(t_k,x_i)})|^2.$$ Reference[1] Ambrosio, Luigi, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces and in the space of probability measures. Basel: Birkhäuser Basel, 2005.","link":"/2025/11/26/flow_matching/"},{"title":"Hello Everyone","text":"Hello everyone! Welcome to my blog! I’ll be sharing my journey as a master student at the Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences. This blog isn’t just about my math research – I’ll also explore topics like economics, finance, and anything else that sparks my interest! If you have any questions or just want to reach out, feel free to email me at wangtuo1020@outlook.com.","link":"/2025/09/11/hello/"}],"tags":[{"name":"Sampling","slug":"Sampling","link":"/tags/Sampling/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Paper Reading","slug":"Paper-Reading","link":"/tags/Paper-Reading/"},{"name":"Generative Modeling","slug":"Generative-Modeling","link":"/tags/Generative-Modeling/"}],"categories":[{"name":"Sampling","slug":"Sampling","link":"/categories/Sampling/"}],"pages":[{"title":"About Me","text":"Hello","link":"/about.html"},{"title":"An Introduction to Sampling","text":"Sampling algorithms provide efficient ways to generate samples from complex probability distributions. They are widely applied in areas such as Bayesian inference, optimization, physics, and machine learning, making them essential tools for connecting theory with practical data analysis. The Goal of SamplingThe goal of sampling is to generate random variable $X\\sim \\pi$ from a target distribution $\\pi$, typically known up to a normalization constant$$\\pi \\propto e^{-V(x)}$$","link":"/sampling_introduction.html"},{"title":"What is Economy?","text":"Three Main Problems for Economy How People Make Decisions How People Interact How the Economy as a Whole Works Ten Principles of EconomyHow People Make Decisions People face tradeoffs. The cost of something is what you give up to get it. Rational people think at the margin. People respond to incentives. How People Interact Trade can make everyone better off. Markets are usually a good way to organize economic activity. Governments can sometimes improve market outcomes. How the Economy as a Whole Works The standard of living depends on a country’s production. Prices rise when the government prints too much money. Society faces a short-run tradeoff between inflation and unempolyment. 1. People face tradeoffsThere is no such thing as a free lunch! To get one thing, we usually have to give up another thing. 2. The cost of something is what you give up to get itThe opportunity cost of an item is what you give up to obtain that item. 3. Rational people think at the marginMarginal changes are small, incremental adjustments to an existing plan of action. 4. People respond to incentivesMarginal changes in costs or benefits motivate people to respond. The decision to choose one alternative over another occurs when that alternative’s marginal benefits exceed its marginal costs! 5. Trade can make everyone better offPeople gain from their ability to trade with one another. Competition results in gains from trading. Trade allows people to specialize in what they do best.","link":"/economy_what_is_economy.html"},{"title":"About Me","text":"Hello","link":"/about/about.html"}]}