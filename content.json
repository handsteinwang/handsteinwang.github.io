{"posts":[{"title":"Introduction to Diffusion Model","text":"Diffusion Model","link":"/2025/10/24/diffusion/"},{"title":"Between Sampling and Optimization","text":"Introduction​ Let $p$ be a positive integer and $f:\\mathbb{R}^p\\to \\mathbb{R}$ be a measurable function such that$$\\int_{\\mathbb{R}^p} e^{-f(x)} dx&lt;+\\infty.$$In various applications, one is faced with the problem of finding the minimum point of $f$ or computing the average with respect to the probability density$$\\pi(x) = \\dfrac{e^{-f(x)}}{\\int_{\\mathbb{R}^p} e^{-f(u)}du}.$$ In other words, one often looks for approximating the values $ x^{*} $ and $ \\bar x $ defined as $$x^{*} \\in \\arg\\min_{x\\in \\mathbb{R}^p} f(x), \\quad \\bar x = \\int_{\\mathbb{R}^p} x \\pi(x) dx.$$ AssumptionAssumption 1. We assume $f$ is strongly convex with Lipschitz continuous gradient. That is, we assume that there exist two positive constants $m$ and $M$ such that for all $x,y\\in \\mathbb{R}^p$,$$f(y)-f(x)-\\nabla f(x)^T (y-x)\\ge \\frac{m}{2}||y-x||_2^2,$$and$$||\\nabla f(x)-\\nabla f(y)||_2\\le M ||x-y||_2.$$ Langevin Monte Carlo (LMC) Algorithm​ Starting from an initial point $x^{(0)}\\in \\mathbb{R}^p$ that maybe deterministic or random, the iterations of the algorithm are defined by the update rule$$x^{(k+1,h)} = x^{(k,h)}-h\\nabla f(x^{(k,h)})+\\sqrt{2h}\\ \\xi^{(k+1)};\\quad k=0,1,2,\\cdots \\tag{LMC}$$where $h&gt;0$ is the tuning parameter, refer to as the step size, and $\\xi^{(1)},\\cdots,\\xi^{(k)},\\cdots$ is a sequence of mutually independent, and independent of $x^{(0)}$, centered Gaussian vectors with covariance matrices equal to identity. Let $\\nu_k$ be the distribution of the $k$-th iterate of the LMC algorithm, that is $x^{(k,h)}\\sim\\nu_k$. Main Idea of LMC Algorithm​ The Markov chain $\\{x^{(k,h)}\\}_{k\\in \\mathbb{N}}$ is the Euler-Maruyama discretization of the following Langevin dynamic SDE:$$dL_t = -\\nabla f(L_t)+\\sqrt{2}dW_t,\\quad t\\ge 0,$$where $\\{W_t;\\ t\\ge 0\\}$ is a $p-$dimensional Brownian motion, $\\{L_t; t\\ge 0\\}$ is known as Langevin diffusion. When $f$ satisfies Assumption 1, the above SDE has a unique strong solution which is a Markov process. ​ By Fokker-Planck Equation, we know $\\{L_t; t\\ge 0\\}$ has $\\pi$ as invariant density, namely if $L_0\\sim \\pi$ then $L_t\\sim \\pi$ for all $t\\ge 0$. ​ Under Assumption 1, when $h$ is small and $k$ is large (so that the product $kh$ is large), the distribution of $x^{(k,h)}$ is close in various metrics to the distribution $\\pi$ with density $\\pi(x)$. So the question is how to quantify this closeness? Wasserstein DistanceDefinition 1. For two measures $\\mu$ and $\\nu$ defined on $(\\mathbb{R}^p,\\mathcal{B}(\\mathbb{R}^p))$, the $2-$Wasserstein distance is defined by$$W_2(\\mu,\\nu) = \\left( \\inf_{\\gamma\\in \\Gamma(\\mu,\\nu)}\\int_{\\mathbb{R}^p\\times \\mathbb{R}^p}||x-y||_2^2d\\gamma(x,y)\\right)^{1/2},$$where the infimum is with respect to all joint distributions $\\gamma$ having $\\mu$ and $\\nu$ as marginal distributions. Here we review some important properties of Wasserstein distance. Proposition 1. For two Dirac measure $\\delta_x$ and $\\delta_y$, we have $$W_2(\\delta_x,\\delta_y) = ||x-y||_2.$$Proposition 2. The infimum in the definition of Wasserstein distance is achievable. That is to say there exists a joint distribution $\\gamma^*\\in \\Gamma(\\mu,\\nu)$ such that $$W_2(\\mu,\\nu) = \\left( \\int_{\\mathbb{R}^p\\times \\mathbb{R}^p}||x-y||_2^2d\\gamma^*(x,y)\\right)^{1/2}.$$ Main Theorem Theorem 1. Assume that $h\\in (0,\\frac{2}{M})$. The following claims hold: (a) If $h\\le \\frac{2}{m+M}$ then $W_2(\\nu_k,\\pi)\\le (1-mh)^k W_2(\\nu_0, \\pi)+ \\frac{M}{m}(5hp/3)^{1/2}.$ (b) If $h\\ge \\frac{2}{m+M}$ then $W_2(\\nu_k,\\pi)\\le (Mh-1)^k W_2(\\nu_0, \\pi)+ \\frac{Mh}{2-Mh}(5hp/3)^{1/2}.$ Relation with OptimizationThe function $f(x)$ and the function $f_\\tau(x):=f(x)/\\tau$ have the same minimizer $x^{*}$, whatever the real number $\\tau&gt;0$. If we define the density function $\\pi_\\tau(x)\\propto \\exp(-f_\\tau(x))$, then the average value$$\\bar x_\\tau = \\int_{\\mathbb{R}^p} x \\pi_\\tau (x)dx$$tends to the minimizer $x^* $ when $\\tau\\to 0$. Furthermore, the distribution $\\pi_{\\tau}$ tends to the Dirac measure $\\delta_{x^{*}}$.","link":"/2025/09/16/LMC_GD/"},{"title":"Introduction to Flow Matching","text":"In generative modeling, we are given a collection of training samples $\\{x_i\\}_{i=1}^N$ and wish to generate new samples from the underlying target distribution $\\pi$. There are already many established approaches to this problem, including likelihood-based methods, implicit generative models such as GANs, and score-based diffusion models. More recently, the flow matching framework has emerged as another powerful paradigm. In what follows, we introduce the basic ideas of flow matching and explain how works. BackgroundFor a particle initially at position $x_0 \\in \\mathbb{R}^d$ and a (Lipschitz continuous) velocity field $\\{v_t\\}$, there exists a unique trajectory $\\{x_t\\}$ described by $$\\begin{cases}\\begin{split}\\dfrac{d x_t}{dt} &amp;= v_t(x_t),\\quad t\\in [0,1]\\\\\\x_0&amp;=x_0\\end{split}\\end{cases}.\\tag{Flow-ODE}$$The flow $\\Phi$ collects the trajectories corresponding to different initial positions $x_0$ and is defined by $$\\begin{split}\\Phi: [0,1]\\times \\mathbb{R}^d&amp;\\to \\mathbb{R}^d\\\\\\(t,x_0)&amp;\\mapsto \\Phi(t,x_0)=x_t,\\end{split}$$where $x_t$ denotes the solution of (Flow-ODE) at time $t$. If the flow mapping $\\Phi$ is known, then for any initial position $x_0 \\in \\mathbb{R}^d$ we can obtain the terminal position $x_1$ via $x_1 = \\Phi(1,x_0)$. To determine the flow mapping $\\Phi$, it suffices to know the velocity field $\\{v_t\\}_{t \\in [0,1]}$ and to solve the ordinary differential equation (Flow-ODE). Figure 1: A particle moves from $x_0$ to $x_1$ along the velocity field $v_t$. In general, we aim to use the flow mapping $\\Phi$ to transport the initial distribution $\\mu_0$ to the target distribution $\\pi$. Figure 2: Transport the initial distribution $\\mu_0$ to the target distribution $\\pi$ by the flow mapping $\\Phi$ (or the corresponding vector field $v_t$). Let $\\{X_t\\}$ be a stochastic process with $\\operatorname{Law}(X_t)=\\mu_t$ such that$$\\begin{cases}\\begin{split}\\dfrac{d X_t}{dt} &amp;= v_t(X_t),\\ a.e. \\\\\\X_0&amp;\\sim \\mu_0\\end{split}\\end{cases},\\quad t\\in [0,1]$$Then we know $(\\mu_t,v_t)$ must satisfies the following continuity equation$$\\partial_t \\mu_t +\\nabla\\cdot (\\mu_tv_t) = 0,$$which means for all $\\varphi\\in C_c^{\\infty}((0,1)\\times \\mathbb{R}^d)$, we have$$\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t(dy)dt=0.\\tag{CE}$$If $\\mu_t$ has density $\\rho_t$ with respect to Lebesgue measure on $\\mathbb{R}^d$, continuity equation can be written as$$\\partial_t \\rho_t +\\nabla\\cdot (\\rho_tv_t) = 0.$$ Note: The ODE formulation$$\\begin{cases}\\begin{split}\\dfrac{d X_t}{dt} &amp;= v_t(X_t),\\ \\text{a.e.} \\\\\\X_0 &amp;\\sim \\mu_0\\end{split}\\end{cases}\\qquad t\\in [0,1]$$is the Lagrangian (particle) viewpoint, while the continuity equation$$\\partial_t \\mu_t + \\nabla\\cdot(\\mu_t v_t) = 0$$is the Eulerian (distribution) viewpoint. One can show that the Lagrangian formulation implies the Eulerian one. Conversely, under suitable assumptions, if the continuity equation holds, then there exists a stochastic process $(X_t)_{t\\in[0,1]}$ solving the ODE above. This result is often referred to as the superposition principle (see [1] for a reference). Hence, once we have learned a “good” flow map $\\Phi$, we can sample $X_0$ from the initial distribution $\\mu_0$ (for instance, a Gaussian $N(0, I_d)$), and then obtain$$\\operatorname{Law}(X_1) =\\operatorname{Law}(\\Phi(1, X_0)) =\\mu_1 \\approx \\pi.$$The central question is therefore: how can we learn such a “good” flow map $\\Phi$, or equivalently, how can we design the vector field $\\{v_t\\}$ using only the training data $\\{x_i\\}_{i=1}^N \\stackrel{\\text{i.i.d.}}{\\sim} \\pi$? Flow MatchingThe main idea of flow matching is to approximate the target vector field $v_t$ by a neural network $u_t^{(\\theta)}$. The neural network is trained by minimizing a suitable discrepancy between $u_t^{(\\theta)}$ and an explicitly known conditional vector field $v_t^x$. To make this precise, we first introduce the conditional probability paths $\\mu_t^x$ and the corresponding conditional vector fields $v_t^x$, and then explain how they relate to the marginal probability path $\\mu_t$ and the marginal vector field $v_t$ that we ultimately wish to learn. Conditional Probability Path and Conditional Vector FieldDefinition 1. The conditional probability path is a path of Markov kernel denoted by $\\mu_t^x$ satisfying: (1) For all $t\\in [0,1]$ and $A\\in \\mathcal{B}(\\mathbb{R}^d)$, the mapping $x\\mapsto \\mu_t^x(A)$ is $\\mathcal{B}(\\mathbb{R}^d)$-measurable; (2) For all $t\\in [0,1]$ and $x\\in \\mathbb{R}^d$, $\\mu_t^x(\\cdot)$ is a probability measure on $\\mathbb{R}^d$; (3) For all $x\\in \\mathbb{R}^d$, $\\mu_0^x\\equiv \\mu_0$ and $\\mu_1^x=\\delta_x$. Note: Property (3) above is used to ensure that the marginal probability path$$\\mu_t(A) := \\int_{\\mathbb{R}^d} \\mu_t^x(A)\\ \\pi(dx), \\quad \\forall A \\in \\mathcal{B}(\\mathbb{R}^d),$$satisfies $\\mu_0 = \\mu_0$ (our prescribed initial distribution) and$$\\mu_1(A) = \\int_{\\mathbb{R}^d} \\delta_x(A)\\ \\pi(dx) = \\int_A \\pi(dx) = \\pi(A), \\quad \\forall A \\in \\mathcal{B}(\\mathbb{R}^d),$$which implies that $\\mu_1 = \\pi$. Hence $\\mu_t$ is a probability path interpolating between the initial distribution $\\mu_0$ and the target distribution $\\pi$, which is exactly what we want. In practice, one may choose a sufficiently small $\\sigma_{\\min} &gt; 0$ and set $\\mu_1^x = N(x, \\sigma_{\\min}^2)$, so that in the end $\\mu_1 \\approx \\pi$. Construction of Conditional Probability PathThere are many ways to construct a conditional probability path $\\mu_t^x$ satisfying the conditions in Definition 1. Here, we adopt a simple conditional Gaussian path$$\\mu_t^x = N(m_t^x,(\\sigma_t^x)^2I_d),$$where$$m_t^x = tx\\quad \\text{and}\\quad (\\sigma_t^x)^2 = (1-t)^2\\sigma_0^2+\\sigma_{\\min}^2,$$with $\\sigma_0^2=1-\\sigma_{\\min}^2$. It is easy to see that$$\\mu_0^x = N(0,I_d)\\quad \\text{and}\\quad \\mu_1^x = N(x,\\sigma_{\\min}^2I_d)\\approx \\delta_x.$$Naturally, we hope our trajectory is$$Y_t^x = m_t^x+\\sigma_t^x Z,$$where $Z\\sim N(0,I_d)$ and we have $\\operatorname{Law}(Y_t^x)=\\mu_t^x$. By differentiating $Y_t^x$ with respect to $t$, we obtain$$\\begin{aligned}\\dfrac{d}{dt}Y_t^x &amp;= \\dot m_t^x+\\dot \\sigma_t^x Z\\\\\\&amp;=\\dot m_t^x+\\dfrac{\\dot \\sigma_t^x}{\\sigma_t^x} (Y_t^x-m_t^x),\\end{aligned}$$where $\\dot m_t^x = \\dfrac{d}{dt} m_t^x$ and $\\dot \\sigma_t^x = \\dfrac{d}{dt} \\sigma_t^x$. Hence, we can choose the conditional vector field$$v_t^x(y):= \\dot m_t^x+\\dfrac{\\dot \\sigma_t^x}{\\sigma_t^x} (y-m_t^x).$$Then we have$$\\begin{cases}\\dfrac{d}{dt}Y_t^x = v_t^x(Y_t^x),\\ \\operatorname{Law}(Y_t^x)=\\mu_t^x\\\\\\Y_0^x\\sim \\mu_0^x\\end{cases},\\quad t\\in [0,1].$$Therefore, $(\\mu_t^x, v_t^x)$ satisfies the continuity equation: for all $\\varphi\\in C_c^{\\infty}((0,1)\\times \\mathbb{R}^d)$, we have$$\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t^x(dy)dt=0.\\tag{CE-conditional}$$ Summary We construct the conditional probability path$$\\mu_t^x = N(m_t^x,(\\sigma_t^x)^2I_d),$$where$$m_t^x = tx\\quad \\text{and}\\quad (\\sigma_t^x)^2 = (1-t)^2\\sigma_0^2+\\sigma_{\\min}^2,$$with $\\sigma_0^2=1-\\sigma_{\\min}^2$ and it satisfies$$\\mu_0^x = N(0,I_d)\\quad \\text{and}\\quad \\mu_1^x = N(x,\\sigma_{\\min}^2I_d)\\approx \\delta_x.$$Then we construct the conditional vector field$$v_t^x(y):= \\dot m_t^x+\\dfrac{\\dot \\sigma_t^x}{\\sigma_t^x} (y-m_t^x).$$And then $(\\mu_t^x, v_t^x)$ satisfies the continuity equation: for all $\\varphi\\in C_c^{\\infty}((0,1)\\times \\mathbb{R}^d)$, we have$$\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t^x(dy)dt=0.\\tag{CE-conditional}$$ Marginal Probability Path and Marginal Vector FieldWe define the marginal probability path $\\mu_t$ by$$\\mu_t(A) := \\int_{\\mathbb{R}^d} \\mu_t^x(A)\\ \\pi(dx), \\quad \\forall A \\in \\mathcal{B}(\\mathbb{R}^d).$$We have $\\mu_t$ start at our initial distribution $\\mu_0$ and end by$$\\mu_1\\approx \\pi.$$What’s more, we have the following lemma. Lemma 1. For $\\pi-$a.e. $x$, we have $\\mu_t^x\\ll \\mu_t$. Proof : If $\\mu_t(A)=0$, then$$\\int_{\\mathbb{R}^d} \\mu_t^x(A)\\ \\pi(dx)=0.$$Since $\\mu_t^x(A)\\ge 0$, we have for $\\pi-$a.e. $x$, $\\mu_t^x(A)=0$. Hence, for $\\pi-$a.e. $x$, we have $\\mu_t^x\\ll \\mu_t$. By Lemma 1, for $\\pi-$a.e. $x$, the Radon-Nikodym derivative$$\\dfrac{d\\mu_t^x}{d\\mu_t}$$exists. Then we can define the marginal vector field as$$v_t(y):= \\int_{\\mathbb{R}^d} v_t^x(y) \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\pi(dx).$$Note: To some extent, $v_t$ is a weighted average of $v_t^x$. Theorem 2. $(\\mu_t, v_t)$ satisfies the continuity equation. Proof : For all $\\varphi\\in C_c^{\\infty}((0,1)\\times \\mathbb{R}^d)$, we need to show that$$I=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t(dy)dt=0.$$In fact, $I=I_1+I_2$, where$$\\begin{aligned}I_1&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\dfrac{\\partial}{\\partial t}\\varphi(t,y)\\mu_t(dy)dt\\\\\\I_2&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t(y), \\nabla\\varphi(t,y)\\rangle\\mu_t(dy)dt.\\end{aligned}$$By using$$\\mu_t(dy) =\\int_{\\mathbb{R}^d} \\mu_t^x(dy)\\ \\pi(dx)$$and$$v_t(y):= \\int_{\\mathbb{R}^d} v_t^x(y) \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\pi(dx),$$we have$$\\begin{aligned}I_1&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\dfrac{\\partial}{\\partial t}\\varphi(t,y)\\ \\mu_t(dy)\\ dt\\\\\\&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\dfrac{\\partial}{\\partial t}\\varphi(t,y)\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt\\\\\\&amp;\\xlongequal{\\text{Fubini}}\\int_{\\mathbb{R}^d}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\dfrac{\\partial}{\\partial t}\\varphi(t,y)\\ \\mu_t^x(dy)\\ dt\\ \\pi(dx)\\end{aligned}$$and$$\\begin{aligned}I_2&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t(y), \\nabla\\varphi(t,y)\\rangle\\ \\mu_t(dy)\\ dt\\\\\\&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\ \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\pi(dx)\\ \\mu_t(dy)\\ dt\\\\\\&amp;\\xlongequal{\\text{Fubini}}\\int_{\\mathbb{R}^d}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\ \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\mu_t(dy)\\ dt \\ \\pi(dx)\\\\\\&amp;\\xlongequal{\\text{Chain Rule}}\\int_{\\mathbb{R}^d}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\ \\mu_t^x(dy)\\ dt \\ \\pi(dx).\\end{aligned}$$Hence by $(\\mu_t^x, v_t^x)$ satisfies the continuity equation, namely$$\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\right]\\mu_t^x(dy)dt=0$$we get$$I=I_1+I_2 =\\int_{\\mathbb{R}^d}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\left[\\dfrac{\\partial}{\\partial t}\\varphi(t,y)+\\langle v_t^x(y), \\nabla\\varphi(t,y)\\rangle\\right]\\ \\mu_t^x(dy)\\ dt\\ \\pi(dx)=0,$$which means $(\\mu_t, v_t)$ satisfies the continuity equation. By Theorem 2, there exists a stochastic process $X_t$ with $\\operatorname{Law}(X_t)=\\mu_t$ such that$$\\begin{cases}\\begin{split}\\dfrac{d X_t}{dt} &amp;= v_t(X_t),\\ a.e. \\\\\\X_0&amp;\\sim \\mu_0\\end{split}\\end{cases},\\quad t\\in [0,1]$$And $\\mu_t$ start at our initial distribution $\\mu_0$ and end by$$\\mu_1\\approx \\pi.$$ Summary We construct the marginal probability path and marginal vector field by$$\\begin{aligned}\\mu_t(A) &amp;:= \\int_{\\mathbb{R}^d} \\mu_t^x(A)\\ \\pi(dx), \\quad \\forall A \\in \\mathcal{B}(\\mathbb{R}^d).\\\\\\v_t(y)&amp;:= \\int_{\\mathbb{R}^d} v_t^x(y) \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\pi(dx).\\end{aligned}$$We have $(\\mu_t, v_t)$ satisfies the continuity equation, which means there exists a stochastic process $X_t$ with $\\operatorname{Law}(X_t)=\\mu_t$ such that$$\\begin{cases}\\begin{split}\\dfrac{d X_t}{dt} &amp;= v_t(X_t),\\ a.e. \\\\\\X_0&amp;\\sim \\mu_0\\end{split}\\end{cases},\\quad t\\in [0,1].$$And $\\mu_t$ start at our initial distribution $\\mu_0$ and end by$$\\mu_1\\approx \\pi.$$ Now we can answer the question about how to train the neural network $u_t^{(\\theta)}$. TrainingTheorem 3. Define the marginal loss $L(\\theta)$ and conditional loss $J(\\theta)$ by$$\\begin{aligned}L(\\theta)&amp;:= \\int_{0}^{1}\\int_{\\mathbb{R}^d}|v_t(y)-u_t^{(\\theta)}(y)|^2\\ \\mu_t(dy)\\ dt\\\\\\J(\\theta)&amp;:= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}|v_t^x(y)-u_t^{(\\theta)}(y)|^2\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt\\end{aligned}$$We have$$L(\\theta) = J(\\theta)+C,$$where $C$ is a constant independent of $\\theta$. Hence,$$\\arg\\min_{\\theta} L(\\theta)=\\arg\\min_{\\theta} J(\\theta).$$Proof : Expand $L(\\theta),\\ J(\\theta)$ as$$L(\\theta) = A_1-2B_1+C_1,$$where$$A_1 = \\int_{0}^{1}\\int_{\\mathbb{R}^d}|v_t(y)|^2\\ \\mu_t(dy)\\ dt,\\ B_1= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t(y), u_t^{(\\theta)}(y)\\rangle\\ \\mu_t(dy)\\ dt,\\ C_1= \\int_{0}^{1}\\int_{\\mathbb{R}^d}|u_t^{(\\theta)}|^2\\ \\mu_t(dy)\\ dt$$and$$J(\\theta) = A_2-2B_2+C_2,$$where$$A_2= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}|v_t^x(y)|^2\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt,\\ B_2=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), u_t^{(\\theta)}(y)\\rangle\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt,\\ C_2= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}|u_t^{(\\theta)}|^2\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt.$$By using$$\\mu_t(dy) =\\int_{\\mathbb{R}^d} \\mu_t^x(dy)\\ \\pi(dx)$$and$$v_t(y):= \\int_{\\mathbb{R}^d} v_t^x(y) \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\pi(dx),$$we have$$\\begin{aligned}C_1 &amp;= \\int_{0}^{1}\\int_{\\mathbb{R}^d}|u_t^{(\\theta)}|^2\\ \\mu_t(dy)\\ dt\\\\\\&amp;= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}|u_t^{(\\theta)}|^2\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt\\\\\\&amp;=C_2\\end{aligned}$$and$$\\begin{aligned}B_1&amp;= \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\langle v_t(y), u_t^{(\\theta)}(y)\\rangle\\ \\mu_t(dy)\\ dt\\\\\\&amp;=\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), u_t^{(\\theta)}(y)\\rangle\\ \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\pi(dx)\\ \\mu_t(dy)\\ dt\\\\\\&amp;\\xlongequal{\\text{Fubini}}\\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), u_t^{(\\theta)}(y)\\rangle\\ \\dfrac{d\\mu_t^x}{d\\mu_t}(y)\\ \\mu_t(dy)\\ \\pi(dx)\\ dt\\\\\\&amp;\\xlongequal{\\text{Chain Rule}} \\int_{0}^{1}\\int_{\\mathbb{R}^d}\\int_{\\mathbb{R}^d}\\langle v_t^x(y), u_t^{(\\theta)}(y)\\rangle\\ \\mu_t^x(dy)\\ \\pi(dx)\\ dt\\\\\\&amp;=B_2.\\end{aligned}$$Since $A_1$ and $A_2$ are independent of $\\theta$, the desired result follows directly. By Theorem 3, we can sample $\\{t_k\\}_{k=1}^K\\sim U[0,1]$; draw from training dataset $\\{x_i\\}_{i=1}^N$; for all $(t_k, x_i)$, draw $M$ samples $\\{y_m^{(t_k,x_i)}\\}_{m=1}^M\\sim \\mu_t^x=N(m_t^x,(\\sigma_t^x)^xI_d)$. And then train our neural network by$$\\min_\\theta\\quad\\dfrac{1}{KNM}\\sum_{k=1}^K\\sum_{i=1}^N\\sum_{m=1}^M |v_{t_k}^{x_i}(y_m^{(t_k,x_i)})-u_{t_k}^{(\\theta)}(y_m^{(t_k,x_i)})|^2.$$ Summary Our goal is to sample from the target distribution $\\pi$. To this end, we first draw samples (for example $x_0$) from an initial distribution $\\mu_0=N(0,I_d)$ and then apply a trained vector field $u_t^{(\\theta)}$, so that at the terminal time we obtain samples (corresponding $x_1$) distributed (approximately) according to $\\pi$ by solving the following ODE$$\\begin{cases}\\begin{split}\\dfrac{d x_t}{dt} &amp;= u_t^{(\\theta)}(x_t),\\quad t\\in [0,1]\\\\\\x_0&amp;=x_0\\end{split}\\end{cases}$$by numerical method such as the Euler method or the Runge-Kutta method. The way to training the vector field $u_t^{(\\theta)}$​ is by$$\\min_\\theta\\quad\\dfrac{1}{KNM}\\sum_{k=1}^K\\sum_{i=1}^N\\sum_{m=1}^M |v_{t_k}^{x_i}(y_m^{(t_k,x_i)})-u_{t_k}^{(\\theta)}(y_m^{(t_k,x_i)})|^2,$$where $t_k\\sim U[0,1], k=1,\\cdots,K$ and $x_i,\\ i=1,\\cdots,N$ draw from training dataset,$$\\{y_m^{(t_k,x_i)}\\}_{m=1}^M\\sim \\mu_t^x=N(m_t^x,(\\sigma_t^x)^xI_d)$$ and$$v_{t}^{x}(y):= \\dot m_t^x+\\dfrac{\\dot \\sigma_t^x}{\\sigma_t^x} (y-m_t^x)$$with $m_t^x = tx\\quad \\text{and}\\quad (\\sigma_t^x)^2 = (1-t)^2\\sigma_0^2+\\sigma_{\\min}^2$ , $\\sigma_0^2=1-\\sigma_{\\min}^2$. There are mainly three types of errors (terminal error) since $\\mu_1^x=N(x,\\sigma_{\\min}^2I_d)\\approx \\delta_x$, there is an error between $\\mu_1$ and target distribution $\\pi$, but we can control this error by choosing $\\sigma_\\min$ sufficiently small. (discretization error) there is an error when solving the ODE above numerically, but we can control this error by choosing step size $h$ sufficiently small. (flow matching error) there is an error between $u_t^{(\\theta)}$ and true vector field $v_t$, but we can assume the neural network can be trained well enough to make this error small enough. In conclusion: Sampling is as easy as learning the vector field! Reference[1] Ambrosio, Luigi, Nicola Gigli, and Giuseppe Savaré. Gradient flows: in metric spaces and in the space of probability measures. Basel: Birkhäuser Basel, 2005. [2] Lipman, Yaron, et al. “Flow matching for generative modeling.” arXiv preprint arXiv:2210.02747 (2022). [3] Kerrigan, Gavin, Giosue Migliorini, and Padhraic Smyth. “Functional flow matching.” arXiv preprint arXiv:2305.17209 (2023).","link":"/2025/11/26/flow_matching/"},{"title":"《全球视野下的投资机会》（第一章读书笔记）","text":"今天是感恩节，因此我想先用本书的作者，时寒冰先生说过的一句话开头： 感恩已经得到的，不要为没有得到的焦虑 第一章 高盈利的源头抓住财富的密码：顺势而为，正确的时间将钱放在正确的市场小钱是辛苦挣来的，大钱则是靠超前的眼光投资赚得的。然而有的人投资获得了巨大的成功，而有的人却穷其一生，除了累累伤痕，心力交瘁，一无所获，原因可以归纳为一个字：“势”。 所谓“势”，就是资本、资源、人口等流动的方向。 中国经济的真正腾飞始于2001年：中国加入世界贸易组织（WTO），全世界资本开始流入中国。 中国本应借助这种经济高速增长的势头，大力发展高端制造业，全面提升中国的科技水平和制造业水平，但房地产的过度投机犹如黑洞，将不计其数的宝贵资源吸入其中，在一定程度上制约了中国的产业技术升级。这也改变了大趋势。 2018年中美贸易战开始，逆全球化浪潮逐渐兴起，逐渐与欧美等发达国家脱钩断链。高端产业回流欧美日等发达国家和地区，低端产业向东南亚、墨西哥等发达国家转移。 2020年的新冠疫情强化了这一趋势。印度、日本等国股市持续大幅度上涨，但日本的制造业体系更为完善，供应链也极为完整，而且在技术、劳动力等方面有明显的优势，因而日本的重要性凸显，相应的日本的股市和楼市得到了强有力的支撑。 资本流入之地，无论是股市还是楼市，都容易上涨。而资本流出之地，无论是股市还是楼市，都容易下跌。 从行业角度看，资本流动的方向同样是具有决定性的主导力量。 人工智能刚进入初始阶段，未来还有很长一段路要走。2022年底后，AI飞速发展，使得人类从石油驱动经济发展时代逐步步入电力驱动经济发展时代。在过去石油有多重要，未来电力就有多重要。 机器人的发展尤其值得重视。制造机器人的材料需要靓号得机械性能，物理性能和化学性能，因此那些质量轻、强度硬度高、耐磨损、耐腐蚀的材料需求极大，例如PEEK、镁合金、钛合金、铝合金、碳纤维、汝铁硼磁材等。 只有在正确的时间将钱放在正确的市场中才能赚钱并且赚大钱。 正确的市场，就是只要选对品种或者选对企业，哪怕有剧烈的调整，也能很快修复，并持续给你带来可观的投资回报的市场。 错误的市场，就是这个市场已经经过漫长的时间检验，它除了吞噬你的财富或者本钱以外，不能给你带来任何像样的回报。 不要用侥幸和幻想美化现实的残酷，要想获取财富，必须战胜幻想。 紧抓稀缺性物以稀为贵，以稀缺性为目标，是获取高盈利的捷径，但必须考虑时间因素，时间是成就、强化稀缺性的帮手，同时也是摧毁、击败稀缺性的利器。 投资上的稀缺性是指：在某个时间段内（时间长短可由稀缺性的强度而定），某种商品的需求稳定或增长，而供应无法同步增长甚至还可能减少的特性，并且，这种商品在某个阶段内缺乏充分的替代商品，从而导致该商品的供给在该时间段内无法满足需求（消费需求与投资需求）。 供给有限 缺少替代商品或者替代商品不足 需求稳定或增长 强调某个时间段内 稀缺性的分类： 地域稀缺性：某种商品只有这个地方有，而别的地方没有，那么这种稀缺性从空间角度看就是无可替代的。例如茅台。 技术稀缺性：带有新明的垄断特色。例如：英伟达、微软、谷歌等公司，反例如英特尔。技术不仅制造稀缺性也摧毁稀缺性，例如钻石。 不可再生稀缺性：指这种资源原本就很少，而且随着人们的开采、使用会越来越少。但是某段时间内，供应的增长会摧毁稀缺性。例如铟、稀土、可可等。 库存增加，意味着稀缺性的基础在消费降级阶段非常容易坍塌，要顺势而为，及时调整。 稀缺性不仅仅适用于投资，也同样适用于做实业和做人。在人心冷漠的时候，用良心做事所具有的稀缺性，就是最大的号召力，会带来好的回报。 与确定性趋势同行对普通人来说，判断趋势只需要关注 消费、就业和人口。 消费是最直接反应经济现状和未来前景的指标：当一个国家的人满世界抢购奢侈品的时候，这个国家处于明确的上升周期，经济增长势头强劲，房价也会因为获得强大的支持而猛烈上涨。当消费不断降级，人们对商品价格斤斤计较，甚至生活必需品的消费都开始缩减的时候，这说明整个经济处于收缩阶段，此时房价，股市，汇率等承受巨大压力，投资容易亏损。 人口也是判断投资趋势时非常重要的一个因素：一切经济活动归根结底围绕着人进行，生产的东西要卖给人消费，建造的房子要卖给人住。人口如果是增长的，经济上行的动力就更足，反之亦然。由于人口惯性，总人口的萎缩会滞后于出生人口的萎缩，因而不能及时反映人口衰减的严峻性以及其对经济社会的深刻影响。如果只按目前这种力度鼓励生育，中国毫无疑问将在几十年内成为老龄化程度和人口萎缩最严重的国家之一。人口的趋势一旦转变，就很难逆转。人口的变化是房价的一个根本性影响因素。","link":"/2025/11/27/global_investment/"},{"title":"Hello Everyone","text":"Hello everyone! Welcome to my blog! I’ll be sharing my journey as a master student at the Academy of Mathematics and Systems Sciences, Chinese Academy of Sciences. This blog isn’t just about my math research – I’ll also explore topics like economics, finance, and anything else that sparks my interest! If you have any questions or just want to reach out, feel free to email me at wangtuo1020@outlook.com.","link":"/2025/09/11/hello/"},{"title":"Introduction to Sampling","text":"There are mainly two settings of sampling problem.","link":"/2025/10/20/sampling_introduction/"},{"title":"Analogy between Sampling and Optimization","text":"Analogy between Sampling and Optimization","link":"/2025/09/26/sampling_optimization/"}],"tags":[{"name":"Sampling","slug":"Sampling","link":"/tags/Sampling/"},{"name":"Diffusion Model","slug":"Diffusion-Model","link":"/tags/Diffusion-Model/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Paper Reading","slug":"Paper-Reading","link":"/tags/Paper-Reading/"},{"name":"Generative Modeling","slug":"Generative-Modeling","link":"/tags/Generative-Modeling/"},{"name":"Investment","slug":"Investment","link":"/tags/Investment/"},{"name":"Book Reading","slug":"Book-Reading","link":"/tags/Book-Reading/"}],"categories":[{"name":"Sampling","slug":"Sampling","link":"/categories/Sampling/"},{"name":"Investment","slug":"Investment","link":"/categories/Investment/"}],"pages":[{"title":"About Me","text":"Hello","link":"/about.html"},{"title":"What is Economy?","text":"Three Main Problems for Economy How People Make Decisions How People Interact How the Economy as a Whole Works Ten Principles of EconomyHow People Make Decisions People face tradeoffs. The cost of something is what you give up to get it. Rational people think at the margin. People respond to incentives. How People Interact Trade can make everyone better off. Markets are usually a good way to organize economic activity. Governments can sometimes improve market outcomes. How the Economy as a Whole Works The standard of living depends on a country’s production. Prices rise when the government prints too much money. Society faces a short-run tradeoff between inflation and unempolyment. 1. People face tradeoffsThere is no such thing as a free lunch! To get one thing, we usually have to give up another thing. 2. The cost of something is what you give up to get itThe opportunity cost of an item is what you give up to obtain that item. 3. Rational people think at the marginMarginal changes are small, incremental adjustments to an existing plan of action. 4. People respond to incentivesMarginal changes in costs or benefits motivate people to respond. The decision to choose one alternative over another occurs when that alternative’s marginal benefits exceed its marginal costs! 5. Trade can make everyone better offPeople gain from their ability to trade with one another. Competition results in gains from trading. Trade allows people to specialize in what they do best.","link":"/economy_what_is_economy.html"},{"title":"About Me","text":"Hello","link":"/about/about.html"},{"title":"An Introduction to Sampling","text":"Sampling algorithms provide efficient ways to generate samples from complex probability distributions. They are widely applied in areas such as Bayesian inference, optimization, physics, and machine learning, making them essential tools for connecting theory with practical data analysis. The Goal of SamplingThe goal of sampling is to generate random variable $X\\sim \\pi$ from a target distribution $\\pi$, typically known up to a normalization constant$$\\pi \\propto e^{-V(x)}$$","link":"/sampling_introduction.html"}]}