<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Introduction to Flow Matching - Handstein Wang</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Handstein Wang"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Handstein Wang"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="In generative modeling, we are given a collection of training samples $\{x_i\}_{i&amp;#x3D;1}^N$ and wish to generate new samples from the underlying target distribution $\pi$. There are already many establish"><meta property="og:type" content="blog"><meta property="og:title" content="Introduction to Flow Matching"><meta property="og:url" content="https://handsteinwang.github.io/2025/11/27/flow_matching/"><meta property="og:site_name" content="Handstein Wang"><meta property="og:description" content="In generative modeling, we are given a collection of training samples $\{x_i\}_{i&amp;#x3D;1}^N$ and wish to generate new samples from the underlying target distribution $\pi$. There are already many establish"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://handsteinwang.github.io/images/greatoceanroad.jpg"><meta property="article:published_time" content="2025-11-26T16:00:00.000Z"><meta property="article:modified_time" content="2025-11-26T16:00:00.000Z"><meta property="article:author" content="Handstein Wang"><meta property="article:tag" content="Sampling"><meta property="article:tag" content="Generative Modeling"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://handsteinwang.github.io/images/greatoceanroad.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://handsteinwang.github.io/2025/11/27/flow_matching/"},"headline":"Introduction to Flow Matching","image":["https://handsteinwang.github.io/images/greatoceanroad.jpg"],"datePublished":"2025-11-26T16:00:00.000Z","dateModified":"2025-11-26T16:00:00.000Z","author":{"@type":"Person","name":"Handstein Wang"},"publisher":{"@type":"Organization","name":"Handstein Wang","logo":{"@type":"ImageObject","url":"https://handsteinwang.github.io/images/hw.svg"}},"description":"In generative modeling, we are given a collection of training samples $\\{x_i\\}_{i&#x3D;1}^N$ and wish to generate new samples from the underlying target distribution $\\pi$. There are already many establish"}</script><link rel="canonical" href="https://handsteinwang.github.io/2025/11/27/flow_matching/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/hw.svg" alt="Handstein Wang" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://wangtuo2002.github.io/">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/images/greatoceanroad.jpg" alt="Introduction to Flow Matching"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-11-26T16:00:00.000Z" title="2025/11/27 上午12:00:00">2025-11-27</time></span><span class="level-item"><a class="link-muted" href="/categories/Sampling/">Sampling</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Introduction to Flow Matching</h1><div class="content"><p>In generative modeling, we are given a collection of training samples $\{x_i\}_{i=1}^N$ and wish to generate new samples from the underlying target distribution $\pi$. There are already many established approaches to this problem, including likelihood-based methods, implicit generative models such as GANs, and score-based diffusion models. More recently, the flow matching framework has emerged as another powerful paradigm. In what follows, we introduce the basic ideas of flow matching and explain how works.</p>
<span id="more"></span>    

<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>For a particle initially at position $x_0 \in \mathbb{R}^d$ and a (Lipschitz continuous) velocity field $\{v_t\}$,  there exists a unique trajectory $\{x_t\}$ described by</p>
<p>$$<br>\begin{cases}<br>\begin{split}<br>\dfrac{d x_t}{dt} &amp;= v_t(x_t),\quad t\in [0,1]\\\<br>x_0&amp;=x_0<br>\end{split}<br>\end{cases}.\tag{Flow-ODE}<br>$$<br>The flow $\Phi$ collects the trajectories corresponding to different initial positions $x_0$ and is defined by</p>
<p>$$<br>\begin{split}<br>\Phi: [0,1]\times \mathbb{R}^d&amp;\to \mathbb{R}^d\\\<br>(t,x_0)&amp;\mapsto \Phi(t,x_0)=x_t,<br>\end{split}<br>$$<br>where $x_t$ denotes the solution of (Flow-ODE) at time $t$.</p>
<p>If the flow mapping $\Phi$ is known, then for any initial position $x_0 \in \mathbb{R}^d$ we can obtain the terminal position $x_1$ via $x_1 = \Phi(1,x_0)$. To determine the flow mapping $\Phi$, it suffices to know the velocity field $\{v_t\}_{t \in [0,1]}$ and to solve the ordinary differential equation (Flow-ODE).</p>
<div align="center">
  <img src="/images/flow.png" style="zoom:33%; display:block; margin:0 auto;" /><br/>
  Figure 1: A particle moves from $x_0$ to $x_1$ along the velocity field $v_t$.
</div>




<p>In general, we aim to use the flow mapping $\Phi$ to transport the initial distribution $\mu_0$ to the target distribution $\pi$.</p>
<div align="center">
  <img src="/images/flow_distribution.png" style="zoom:33%; display:block; margin:0 auto;" /><br/>
  Figure 2: Transport the initial distribution $\mu_0$ to the target distribution $\pi$ by the flow mapping $\Phi$ (or the corresponding vector field $v_t$).
</div>





<p>Let $\{X_t\}$ be a stochastic process with $\operatorname{Law}(X_t)=\mu_t$ such that<br>$$<br>\begin{cases}<br>\begin{split}<br>\dfrac{d X_t}{dt} &amp;= v_t(X_t),\ a.e. \\\<br>X_0&amp;\sim \mu_0<br>\end{split}<br>\end{cases},\quad t\in [0,1]<br>$$<br>Then we know $(\mu_t,v_t)$ must satisfies the following <strong>continuity equation</strong><br>$$<br>\partial_t \mu_t +\nabla\cdot (\mu_tv_t) = 0,<br>$$<br>which means for all $\varphi\in C_c^{\infty}((0,1)\times \mathbb{R}^d)$, we have<br>$$<br>\int_{0}^{1}\int_{\mathbb{R}^d}\left[\dfrac{\partial}{\partial t}\varphi(t,y)+\langle v_t(y), \nabla\varphi(t,y)\rangle\right]\mu_t(dy)dt=0.\tag{CE}<br>$$<br>If $\mu_t$ has density $\rho_t$ with respect to Lebesgue measure on $\mathbb{R}^d$, continuity equation can be written as<br>$$<br>\partial_t \rho_t +\nabla\cdot (\rho_tv_t) = 0.<br>$$</p>
<blockquote>
<p>Note: The ODE formulation<br>$$<br>\begin{cases}<br>\begin{split}<br>\dfrac{d X_t}{dt} &amp;= v_t(X_t),\ \text{a.e.} \\\<br>X_0 &amp;\sim \mu_0<br>\end{split}<br>\end{cases}\qquad t\in [0,1]<br>$$<br>is the <strong>Lagrangian</strong> (particle) viewpoint, while the continuity equation<br>$$<br>\partial_t \mu_t + \nabla\cdot(\mu_t v_t) = 0<br>$$<br>is the <strong>Eulerian</strong> (distribution) viewpoint. One can show that the Lagrangian formulation implies the Eulerian one. Conversely, under suitable assumptions, if the continuity equation holds, then there exists a stochastic process $(X_t)_{t\in[0,1]}$ solving the ODE above. This result is often referred to as the <em>superposition principle</em> (see [1] for a reference).</p>
</blockquote>
<p>Hence, once we have learned a “good” flow map $\Phi$, we can sample $X_0$ from the initial distribution $\mu_0$ (for instance, a Gaussian $N(0, I_d)$), and then obtain<br>$$<br>\operatorname{Law}(X_1) =\operatorname{Law}(\Phi(1, X_0)) =\mu_1 \approx \pi.<br>$$<br>The central question is therefore: how can we learn such a “good” flow map $\Phi$, or equivalently, how can we design the vector field $\{v_t\}$ using only the training data $\{x_i\}_{i=1}^N \stackrel{\text{i.i.d.}}{\sim} \pi$?</p>
<h2 id="Flow-Matching"><a href="#Flow-Matching" class="headerlink" title="Flow Matching"></a>Flow Matching</h2><p>The main idea of flow matching is to approximate the target vector field $v_t$ by a neural network $u_t^{(\theta)}$. The neural network is trained by minimizing a suitable discrepancy between $u_t^{(\theta)}$ and an explicitly known conditional vector field $v_t^x$. To make this precise, we first introduce the conditional probability paths $\mu_t^x$ and the corresponding conditional vector fields $v_t^x$, and then explain how they relate to the marginal probability path $\mu_t$ and the marginal vector field $v_t$ that we ultimately wish to learn.</p>
<h3 id="Conditional-Probability-Path-and-Conditional-Vector-Field"><a href="#Conditional-Probability-Path-and-Conditional-Vector-Field" class="headerlink" title="Conditional Probability Path and Conditional Vector Field"></a>Conditional Probability Path and Conditional Vector Field</h3><p><strong>Definition 1.</strong> The conditional probability path is a path of Markov kernel denoted by $\mu_t^x$ satisfying:</p>
<p>(1) For all $t\in [0,1]$ and $A\in \mathcal{B}(\mathbb{R}^d)$, the mapping $x\mapsto \mu_t^x(A)$ is $\mathcal{B}(\mathbb{R}^d)$-measurable;</p>
<p>(2) For all $t\in [0,1]$ and $x\in \mathbb{R}^d$, $\mu_t^x(\cdot)$ is a probability measure on $\mathbb{R}^d$;</p>
<p>(3) For all  $x\in \mathbb{R}^d$, $\mu_0^x\equiv \mu_0$ and $\mu_1^x=\delta_x$.</p>
<p><strong>Note:</strong> Property (3) above is used to ensure that the marginal probability path<br>$$<br>\mu_t(A) := \int_{\mathbb{R}^d} \mu_t^x(A)\ \pi(dx), \quad \forall A \in \mathcal{B}(\mathbb{R}^d),<br>$$<br>satisfies $\mu_0 = \mu_0$ (our prescribed initial distribution) and<br>$$<br>\mu_1(A) = \int_{\mathbb{R}^d} \delta_x(A)\ \pi(dx) = \int_A \pi(dx) = \pi(A), \quad \forall A \in \mathcal{B}(\mathbb{R}^d),<br>$$<br>which implies that $\mu_1 = \pi$. Hence $\mu_t$ is a probability path interpolating between the initial distribution $\mu_0$ and the target distribution $\pi$, which is exactly what we want.</p>
<p>In practice, one may choose a sufficiently small $\sigma_{\min} &gt; 0$ and set $\mu_1^x = N(x, \sigma_{\min}^2)$, so that in the end $\mu_1 \approx \pi$.</p>
<h4 id="Construction-of-Conditional-Probability-Path"><a href="#Construction-of-Conditional-Probability-Path" class="headerlink" title="Construction of Conditional Probability Path"></a>Construction of Conditional Probability Path</h4><p>There are many ways to construct a conditional probability path $\mu_t^x$ satisfying the conditions in Definition 1. Here, we adopt a simple conditional Gaussian path<br>$$<br>\mu_t^x = N(m_t^x,(\sigma_t^x)^2I_d),<br>$$<br>where<br>$$<br>m_t^x = tx\quad \text{and}\quad (\sigma_t^x)^2 = (1-t)^2\sigma_0^2+\sigma_{\min}^2,<br>$$<br>with $\sigma_0^2=1-\sigma_{\min}^2$. It is easy to see that<br>$$<br>\mu_0^x = N(0,I_d)\quad \text{and}\quad \mu_1^x = N(x,\sigma_{\min}^2I_d)\approx \delta_x.<br>$$<br>Naturally, we hope our trajectory is<br>$$<br>Y_t^x = m_t^x+\sigma_t^x Z,<br>$$<br>where $Z\sim N(0,I_d)$ and we have $\operatorname{Law}(Y_t^x)=\mu_t^x$. By differentiating $Y_t^x$ with respect to $t$, we obtain<br>$$<br>\begin{aligned}<br>\dfrac{d}{dt}Y_t^x &amp;= \dot m_t^x+\dot \sigma_t^x Z\\\<br>&amp;=\dot m_t^x+\dfrac{\dot \sigma_t^x}{\sigma_t^x} (Y_t^x-m_t^x),<br>\end{aligned}<br>$$<br>where $\dot m_t^x = \dfrac{d}{dt} m_t^x$ and $\dot \sigma_t^x = \dfrac{d}{dt} \sigma_t^x$. Hence, we can choose the conditional vector field<br>$$<br>v_t^x(y):= \dot m_t^x+\dfrac{\dot \sigma_t^x}{\sigma_t^x} (y-m_t^x).<br>$$<br>Then we have<br>$$<br>\begin{cases}<br>\dfrac{d}{dt}Y_t^x = v_t^x(Y_t^x),\ \operatorname{Law}(Y_t^x)=\mu_t^x\\\<br>Y_0^x\sim \mu_0^x<br>\end{cases},\quad t\in [0,1].<br>$$<br>Therefore, $(\mu_t^x, v_t^x)$ satisfies the continuity equation: for all $\varphi\in C_c^{\infty}((0,1)\times \mathbb{R}^d)$, we have<br>$$<br>\int_{0}^{1}\int_{\mathbb{R}^d}\left[\dfrac{\partial}{\partial t}\varphi(t,y)+\langle v_t^x(y), \nabla\varphi(t,y)\rangle\right]\mu_t^x(dy)dt=0.\tag{CE-conditional}<br>$$</p>
<blockquote>
<p><strong>Summary</strong></p>
<p>We construct the conditional probability path<br>$$<br>\mu_t^x = N(m_t^x,(\sigma_t^x)^2I_d),<br>$$<br>where<br>$$<br>m_t^x = tx\quad \text{and}\quad (\sigma_t^x)^2 = (1-t)^2\sigma_0^2+\sigma_{\min}^2,<br>$$<br>with $\sigma_0^2=1-\sigma_{\min}^2$ and it satisfies<br>$$<br>\mu_0^x = N(0,I_d)\quad \text{and}\quad \mu_1^x = N(x,\sigma_{\min}^2I_d)\approx \delta_x.<br>$$<br>Then we construct the conditional vector field<br>$$<br>v_t^x(y):= \dot m_t^x+\dfrac{\dot \sigma_t^x}{\sigma_t^x} (y-m_t^x).<br>$$<br>And then  $(\mu_t^x, v_t^x)$ satisfies the continuity equation: for all $\varphi\in C_c^{\infty}((0,1)\times \mathbb{R}^d)$, we have<br>$$<br>\int_{0}^{1}\int_{\mathbb{R}^d}\left[\dfrac{\partial}{\partial t}\varphi(t,y)+\langle v_t^x(y), \nabla\varphi(t,y)\rangle\right]\mu_t^x(dy)dt=0.\tag{CE-conditional}<br>$$</p>
</blockquote>
<h3 id="Marginal-Probability-Path-and-Marginal-Vector-Field"><a href="#Marginal-Probability-Path-and-Marginal-Vector-Field" class="headerlink" title="Marginal Probability Path and Marginal Vector Field"></a>Marginal Probability Path and Marginal Vector Field</h3><p>We define the marginal probability path $\mu_t$ by<br>$$<br>\mu_t(A) := \int_{\mathbb{R}^d} \mu_t^x(A)\ \pi(dx), \quad \forall A \in \mathcal{B}(\mathbb{R}^d).<br>$$<br>We have $\mu_t$ start at our initial distribution $\mu_0$ and end by<br>$$<br>\mu_1\approx \pi.<br>$$<br>What’s more, we have the following lemma.</p>
<p><strong>Lemma 1.</strong> For $\pi-$a.e. $x$, we have $\mu_t^x\ll \mu_t$.</p>
<p><strong>Proof :</strong> If $\mu_t(A)=0$, then<br>$$<br>\int_{\mathbb{R}^d} \mu_t^x(A)\ \pi(dx)=0.<br>$$<br>Since $\mu_t^x(A)\ge 0$, we have for $\pi-$a.e. $x$, $\mu_t^x(A)=0$. Hence, for $\pi-$a.e. $x$, we have $\mu_t^x\ll \mu_t$.</p>
<p>By Lemma 1, for $\pi-$a.e. $x$, the Radon-Nikodym derivative<br>$$<br>\dfrac{d\mu_t^x}{d\mu_t}<br>$$<br>exists. Then we can define the marginal vector field as<br>$$<br>v_t(y):= \int_{\mathbb{R}^d} v_t^x(y) \dfrac{d\mu_t^x}{d\mu_t}(y)\pi(dx).<br>$$<br>Note: To some extent, $v_t$ is a weighted average of $v_t^x$.</p>
<p><strong>Theorem 2.</strong> $(\mu_t, v_t)$ satisfies the continuity equation.</p>
<p><strong>Proof :</strong> For all $\varphi\in C_c^{\infty}((0,1)\times \mathbb{R}^d)$, we need to show that<br>$$<br>I=\int_{0}^{1}\int_{\mathbb{R}^d}\left[\dfrac{\partial}{\partial t}\varphi(t,y)+\langle v_t(y), \nabla\varphi(t,y)\rangle\right]\mu_t(dy)dt=0.<br>$$<br>In fact, $I=I_1+I_2$, where<br>$$<br>\begin{aligned}<br>I_1&amp;=\int_{0}^{1}\int_{\mathbb{R}^d}\dfrac{\partial}{\partial t}\varphi(t,y)\mu_t(dy)dt\\\<br>I_2&amp;=\int_{0}^{1}\int_{\mathbb{R}^d}\langle v_t(y), \nabla\varphi(t,y)\rangle\mu_t(dy)dt.<br>\end{aligned}<br>$$<br>By using<br>$$<br>\mu_t(dy) =\int_{\mathbb{R}^d} \mu_t^x(dy)\ \pi(dx)<br>$$<br>and<br>$$<br>v_t(y):= \int_{\mathbb{R}^d} v_t^x(y) \dfrac{d\mu_t^x}{d\mu_t}(y)\ \pi(dx),<br>$$<br>we have<br>$$<br>\begin{aligned}<br>I_1&amp;=\int_{0}^{1}\int_{\mathbb{R}^d}\dfrac{\partial}{\partial t}\varphi(t,y)\ \mu_t(dy)\ dt\\\<br>&amp;=\int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}\dfrac{\partial}{\partial t}\varphi(t,y)\ \mu_t^x(dy)\ \pi(dx)\ dt\\\<br>&amp;\xlongequal{\text{Fubini}}\int_{\mathbb{R}^d}\int_{0}^{1}\int_{\mathbb{R}^d}\dfrac{\partial}{\partial t}\varphi(t,y)\ \mu_t^x(dy)\ dt\ \pi(dx)<br>\end{aligned}<br>$$<br>and<br>$$<br>\begin{aligned}<br>I_2&amp;=\int_{0}^{1}\int_{\mathbb{R}^d}\langle v_t(y), \nabla\varphi(t,y)\rangle\ \mu_t(dy)\ dt\\\<br>&amp;=\int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}\langle v_t^x(y), \nabla\varphi(t,y)\rangle\  \dfrac{d\mu_t^x}{d\mu_t}(y)\ \pi(dx)\  \mu_t(dy)\ dt\\\<br>&amp;\xlongequal{\text{Fubini}}\int_{\mathbb{R}^d}\int_{0}^{1}\int_{\mathbb{R}^d}\langle v_t^x(y), \nabla\varphi(t,y)\rangle\  \dfrac{d\mu_t^x}{d\mu_t}(y)\  \mu_t(dy)\ dt \ \pi(dx)\\\<br>&amp;\xlongequal{\text{Chain Rule}}\int_{\mathbb{R}^d}\int_{0}^{1}\int_{\mathbb{R}^d}\langle v_t^x(y), \nabla\varphi(t,y)\rangle\   \mu_t^x(dy)\ dt \ \pi(dx).<br>\end{aligned}<br>$$<br>Hence by $(\mu_t^x, v_t^x)$ satisfies the continuity equation, namely<br>$$<br>\int_{0}^{1}\int_{\mathbb{R}^d}\left[\dfrac{\partial}{\partial t}\varphi(t,y)+\langle v_t^x(y), \nabla\varphi(t,y)\rangle\right]\mu_t^x(dy)dt=0<br>$$<br>we get<br>$$<br>I=I_1+I_2 =\int_{\mathbb{R}^d}\int_{0}^{1}\int_{\mathbb{R}^d}\left[\dfrac{\partial}{\partial t}\varphi(t,y)+\langle v_t^x(y), \nabla\varphi(t,y)\rangle\right]\ \mu_t^x(dy)\ dt\ \pi(dx)=0,<br>$$<br>which means $(\mu_t, v_t)$ satisfies the continuity equation.</p>
<p>By Theorem 2, there exists a stochastic process $X_t$ with $\operatorname{Law}(X_t)=\mu_t$ such that<br>$$<br>\begin{cases}<br>\begin{split}<br>\dfrac{d X_t}{dt} &amp;= v_t(X_t),\ a.e. \\\<br>X_0&amp;\sim \mu_0<br>\end{split}<br>\end{cases},\quad t\in [0,1]<br>$$<br>And $\mu_t$ start at our initial distribution $\mu_0$ and end by<br>$$<br>\mu_1\approx \pi.<br>$$</p>
<blockquote>
<p><strong>Summary</strong></p>
<p>We construct the marginal probability path and marginal vector field by<br>$$<br>\begin{aligned}<br>\mu_t(A) &amp;:= \int_{\mathbb{R}^d} \mu_t^x(A)\ \pi(dx), \quad \forall A \in \mathcal{B}(\mathbb{R}^d).\\\<br>v_t(y)&amp;:= \int_{\mathbb{R}^d} v_t^x(y) \dfrac{d\mu_t^x}{d\mu_t}(y)\pi(dx).<br>\end{aligned}<br>$$<br>We have $(\mu_t, v_t)$ satisfies the continuity equation, which means there exists a stochastic process $X_t$ with $\operatorname{Law}(X_t)=\mu_t$ such that<br>$$<br>\begin{cases}<br>\begin{split}<br>\dfrac{d X_t}{dt} &amp;= v_t(X_t),\ a.e. \\\<br>X_0&amp;\sim \mu_0<br>\end{split}<br>\end{cases},\quad t\in [0,1].<br>$$<br>And $\mu_t$ start at our initial distribution $\mu_0$ and end by<br>$$<br>\mu_1\approx \pi.<br>$$</p>
</blockquote>
<p>Now we can answer the question about how to train the neural network $u_t^{(\theta)}$.</p>
<h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p><strong>Theorem 3.</strong> Define the marginal loss $L(\theta)$ and conditional loss $J(\theta)$ by<br>$$<br>\begin{aligned}<br>L(\theta)&amp;:= \int_{0}^{1}\int_{\mathbb{R}^d}|v_t(y)-u_t^{(\theta)}(y)|^2\ \mu_t(dy)\ dt\\\<br>J(\theta)&amp;:= \int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}|v_t^x(y)-u_t^{(\theta)}(y)|^2\ \mu_t^x(dy)\ \pi(dx)\ dt<br>\end{aligned}<br>$$<br>We have<br>$$<br>L(\theta) = J(\theta)+C,<br>$$<br>where $C$ is a constant independent of $\theta$. Hence,<br>$$<br>\arg\min_{\theta} L(\theta)=\arg\min_{\theta} J(\theta).<br>$$<br><strong>Proof :</strong> Expand $L(\theta),\ J(\theta)$ as<br>$$<br>L(\theta) = A_1-2B_1+C_1,<br>$$<br>where<br>$$<br>A_1 = \int_{0}^{1}\int_{\mathbb{R}^d}|v_t(y)|^2\ \mu_t(dy)\ dt,\ B_1= \int_{0}^{1}\int_{\mathbb{R}^d}\langle v_t(y), u_t^{(\theta)}(y)\rangle\ \mu_t(dy)\ dt,\ C_1= \int_{0}^{1}\int_{\mathbb{R}^d}|u_t^{(\theta)}|^2\ \mu_t(dy)\ dt<br>$$<br>and<br>$$<br>J(\theta) = A_2-2B_2+C_2,<br>$$<br>where<br>$$<br>A_2= \int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}|v_t^x(y)|^2\ \mu_t^x(dy)\ \pi(dx)\  dt,\ B_2=\int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}\langle v_t^x(y), u_t^{(\theta)}(y)\rangle\ \mu_t^x(dy)\ \pi(dx)\ dt,\ C_2= \int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}|u_t^{(\theta)}|^2\ \mu_t^x(dy)\ \pi(dx)\ dt.<br>$$<br>By using<br>$$<br>\mu_t(dy) =\int_{\mathbb{R}^d} \mu_t^x(dy)\ \pi(dx)<br>$$<br>and<br>$$<br>v_t(y):= \int_{\mathbb{R}^d} v_t^x(y) \dfrac{d\mu_t^x}{d\mu_t}(y)\ \pi(dx),<br>$$<br>we have<br>$$<br>\begin{aligned}<br>C_1 &amp;= \int_{0}^{1}\int_{\mathbb{R}^d}|u_t^{(\theta)}|^2\ \mu_t(dy)\ dt\\\<br>&amp;= \int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}|u_t^{(\theta)}|^2\ \mu_t^x(dy)\ \pi(dx)\ dt\\\<br>&amp;=C_2<br>\end{aligned}<br>$$<br>and<br>$$<br>\begin{aligned}<br>B_1&amp;= \int_{0}^{1}\int_{\mathbb{R}^d}\langle v_t(y), u_t^{(\theta)}(y)\rangle\ \mu_t(dy)\ dt\\\<br>&amp;=\int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}\langle v_t^x(y), u_t^{(\theta)}(y)\rangle\  \dfrac{d\mu_t^x}{d\mu_t}(y)\ \pi(dx)\ \mu_t(dy)\ dt\\\<br>&amp;\xlongequal{\text{Fubini}}\int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}\langle v_t^x(y), u_t^{(\theta)}(y)\rangle\  \dfrac{d\mu_t^x}{d\mu_t}(y)\ \mu_t(dy)\ \pi(dx)\ dt\\\<br>&amp;\xlongequal{\text{Chain Rule}} \int_{0}^{1}\int_{\mathbb{R}^d}\int_{\mathbb{R}^d}\langle v_t^x(y), u_t^{(\theta)}(y)\rangle\ \mu_t^x(dy)\ \pi(dx)\ dt\\\<br>&amp;=B_2.<br>\end{aligned}<br>$$<br>Since $A_1$ and $A_2$ are independent of $\theta$, the desired result follows directly.</p>
<p>By Theorem 3, we can</p>
<ul>
<li>sample $\{t_k\}_{k=1}^K\sim U[0,1]$;</li>
<li>draw from training dataset $\{x_i\}_{i=1}^N$;</li>
<li>for all $(t_k, x_i)$, draw $M$ samples $\{y_m^{(t_k,x_i)}\}_{m=1}^M\sim \mu_t^x=N(m_t^x,(\sigma_t^x)^xI_d)$.</li>
</ul>
<p>And then train our neural network by<br>$$<br>\min_\theta\quad\dfrac{1}{KNM}\sum_{k=1}^K\sum_{i=1}^N\sum_{m=1}^M |v_{t_k}^{x_i}(y_m^{(t_k,x_i)})-u_{t_k}^{(\theta)}(y_m^{(t_k,x_i)})|^2.<br>$$</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><blockquote>
<p>Our goal is to sample from the target distribution $\pi$. To this end, we first draw samples (for example $x_0$)  from an initial distribution $\mu_0=N(0,I_d)$ and then apply a trained vector field $u_t^{(\theta)}$, so that at the terminal time we obtain samples (corresponding $x_1$) distributed (approximately) according to $\pi$ by solving the following ODE<br>$$<br>\begin{cases}<br>\begin{split}<br>\dfrac{d x_t}{dt} &amp;= u_t^{(\theta)}(x_t),\quad t\in [0,1]\\\<br>x_0&amp;=x_0<br>\end{split}<br>\end{cases}<br>$$<br>by numerical method such as the Euler method or the Runge-Kutta method. The way to training the  vector field $u_t^{(\theta)}$​ is by<br>$$<br>\min_\theta\quad\dfrac{1}{KNM}\sum_{k=1}^K\sum_{i=1}^N\sum_{m=1}^M |v_{t_k}^{x_i}(y_m^{(t_k,x_i)})-u_{t_k}^{(\theta)}(y_m^{(t_k,x_i)})|^2,<br>$$<br>where $t_k\sim U[0,1], k=1,\cdots,K$  and $x_i,\ i=1,\cdots,N$ draw from training dataset,<br>$$<br>\{y_m^{(t_k,x_i)}\}_{m=1}^M\sim \mu_t^x=N(m_t^x,(\sigma_t^x)^xI_d)<br>$$</p>
<p>and<br>$$<br>v_{t}^{x}(y):= \dot m_t^x+\dfrac{\dot \sigma_t^x}{\sigma_t^x} (y-m_t^x)<br>$$<br>with $m_t^x = tx\quad \text{and}\quad (\sigma_t^x)^2 = (1-t)^2\sigma_0^2+\sigma_{\min}^2$ , $\sigma_0^2=1-\sigma_{\min}^2$.</p>
<p>There are mainly three types of errors</p>
<ul>
<li>(terminal error) since $\mu_1^x=N(x,\sigma_{\min}^2I_d)\approx \delta_x$, there is an error between $\mu_1$ and target distribution $\pi$, but we can control this error by choosing $\sigma_\min$ sufficiently small.</li>
<li>(discretization error) there is an error when solving the ODE above numerically, but we can control this error by choosing step size $h$ sufficiently small.</li>
<li>(flow matching error) there is an error between $u_t^{(\theta)}$ and true vector field $v_t$, but we can assume the neural network can be trained well enough to make this error small enough.</li>
</ul>
<p>In conclusion: Sampling is as easy as learning the vector field!</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Ambrosio, Luigi, Nicola Gigli, and Giuseppe Savaré. <em>Gradient flows: in metric spaces and in the space of probability measures</em>. Basel: Birkhäuser Basel, 2005.</p>
<p>[2] Lipman, Yaron, et al. “Flow matching for generative modeling.” <em>arXiv preprint arXiv:2210.02747</em> (2022).</p>
<p>[3] Kerrigan, Gavin, Giosue Migliorini, and Padhraic Smyth. “Functional flow matching.” <em>arXiv preprint arXiv:2305.17209</em> (2023).</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Introduction to Flow Matching</p><p><a href="https://handsteinwang.github.io/2025/11/27/flow_matching/">https://handsteinwang.github.io/2025/11/27/flow_matching/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Handstein Wang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-11-27</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-11-27</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Sampling/">Sampling</a><a class="link-muted mr-2" rel="tag" href="/tags/Generative-Modeling/">Generative Modeling</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2025/12/18/Markov_Chain_1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Reversible, Conductance and Rapid Mixing of Markov Chains</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/10/24/diffusion/"><span class="level-item">Introduction to Diffusion Model</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/quokka.png" alt="Handstein Wang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Handstein Wang</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Australia</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">7</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://wangtuo2002.github.io/" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://wangtuo2002.github.io/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Email" href="mailto:wangtuo1020@outlook.com"><i class="fas fa-envelope"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://sep.ucas.ac.cn/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">SEP-UCAS</span></span><span class="level-right"><span class="level-item tag">sep.ucas.ac.cn</span></span></a></li><li><a class="level is-mobile" href="https://online.amss.ac.cn/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Online-AMSS</span></span><span class="level-right"><span class="level-item tag">online.amss.ac.cn</span></span></a></li><li><a class="level is-mobile" href="https://overleaf.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Overleaf</span></span><span class="level-right"><span class="level-item tag">overleaf.com</span></span></a></li><li><a class="level is-mobile" href="https://mathtube.org/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">MathTube</span></span><span class="level-right"><span class="level-item tag">mathtube.org</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Investment/"><span class="level-start"><span class="level-item">Investment</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Sampling/"><span class="level-start"><span class="level-item">Sampling</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2026-02-15T00:27:29.670Z">2026-02-15</time></p><p class="title"><a href="/2026/02/15/report_2025/">2025资产配置报告</a></p><p class="categories"><a href="/categories/Investment/">Investment</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-12-17T16:00:00.000Z">2025-12-18</time></p><p class="title"><a href="/2025/12/18/Markov_Chain_1/">Reversible, Conductance and Rapid Mixing of Markov Chains</a></p><p class="categories"><a href="/categories/Sampling/">Sampling</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-11-26T16:00:00.000Z">2025-11-27</time></p><p class="title"><a href="/2025/11/27/flow_matching/">Introduction to Flow Matching</a></p><p class="categories"><a href="/categories/Sampling/">Sampling</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-10-23T16:00:00.000Z">2025-10-24</time></p><p class="title"><a href="/2025/10/24/diffusion/">Introduction to Diffusion Model</a></p><p class="categories"><a href="/categories/Sampling/">Sampling</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-10-19T16:00:00.000Z">2025-10-20</time></p><p class="title"><a href="/2025/10/20/sampling_introduction/">Introduction to Sampling</a></p><p class="categories"><a href="/categories/Sampling/">Sampling</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2026/02/"><span class="level-start"><span class="level-item">February 2026</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/12/"><span class="level-start"><span class="level-item">December 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/11/"><span class="level-start"><span class="level-item">November 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/10/"><span class="level-start"><span class="level-item">October 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/09/"><span class="level-start"><span class="level-item">September 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Diffusion-Model/"><span class="tag">Diffusion Model</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Generative-Modeling/"><span class="tag">Generative Modeling</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paper-Reading/"><span class="tag">Paper Reading</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Report/"><span class="tag">Report</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sampling/"><span class="tag">Sampling</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stochastic-Process/"><span class="tag">Stochastic Process</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/hw.svg" alt="Handstein Wang" height="28"></a><p class="is-size-7"><span>&copy; 2026 Handstein Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>