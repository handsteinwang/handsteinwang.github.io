<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Reversible, Conductance and Rapid Mixing of Markov Chains - Handstein Wang</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Handstein Wang"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Handstein Wang"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="IntroductionMarkov ChainLet $(\Omega,\mathcal{F})$ be a measurable space, $P:\Omega\times \mathcal{F}\to [0,1]$ be a transition kernel, that is,  for every $x\in \Omega$, $P(x,\cdot)$ is a probability"><meta property="og:type" content="blog"><meta property="og:title" content="Reversible, Conductance and Rapid Mixing of Markov Chains"><meta property="og:url" content="https://handsteinwang.github.io/2025/12/18/Markov_Chain_1/"><meta property="og:site_name" content="Handstein Wang"><meta property="og:description" content="IntroductionMarkov ChainLet $(\Omega,\mathcal{F})$ be a measurable space, $P:\Omega\times \mathcal{F}\to [0,1]$ be a transition kernel, that is,  for every $x\in \Omega$, $P(x,\cdot)$ is a probability"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://handsteinwang.github.io/images/glenorchy.jpg"><meta property="article:published_time" content="2025-12-17T16:00:00.000Z"><meta property="article:modified_time" content="2025-12-17T16:00:00.000Z"><meta property="article:author" content="Handstein Wang"><meta property="article:tag" content="Sampling"><meta property="article:tag" content="Stochastic Process"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://handsteinwang.github.io/images/glenorchy.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://handsteinwang.github.io/2025/12/18/Markov_Chain_1/"},"headline":"Reversible, Conductance and Rapid Mixing of Markov Chains","image":["https://handsteinwang.github.io/images/glenorchy.jpg"],"datePublished":"2025-12-17T16:00:00.000Z","dateModified":"2025-12-17T16:00:00.000Z","author":{"@type":"Person","name":"Handstein Wang"},"publisher":{"@type":"Organization","name":"Handstein Wang","logo":{"@type":"ImageObject","url":"https://handsteinwang.github.io/images/hw.svg"}},"description":"IntroductionMarkov ChainLet $(\\Omega,\\mathcal{F})$ be a measurable space, $P:\\Omega\\times \\mathcal{F}\\to [0,1]$ be a transition kernel, that is,  for every $x\\in \\Omega$, $P(x,\\cdot)$ is a probability"}</script><link rel="canonical" href="https://handsteinwang.github.io/2025/12/18/Markov_Chain_1/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/hw.svg" alt="Handstein Wang" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" target="_blank" rel="noopener" href="https://wangtuo2002.github.io/">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="/images/glenorchy.jpg" alt="Reversible, Conductance and Rapid Mixing of Markov Chains"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2025-12-17T16:00:00.000Z" title="2025/12/18 上午12:00:00">2025-12-18</time></span><span class="level-item"><a class="link-muted" href="/categories/Sampling/">Sampling</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Reversible, Conductance and Rapid Mixing of Markov Chains</h1><div class="content"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Markov-Chain"><a href="#Markov-Chain" class="headerlink" title="Markov Chain"></a>Markov Chain</h3><p>Let $(\Omega,\mathcal{F})$ be a measurable space, $P:\Omega\times \mathcal{F}\to [0,1]$ be a transition kernel, that is,</p>
<ul>
<li>for every $x\in \Omega$, $P(x,\cdot)$ is a probability measure on $\mathcal{F}$;</li>
<li>for every $A\in \mathcal{F}$, $x\mapsto P(x,A)$ is measurable.</li>
</ul>
<span id="more"></span>    

<p>And we say the triple $(\Omega,\mathcal F,P)$ is a <strong>Markov scheme</strong>.</p>
<p>Fix an initial distribution $\mu_0$ on $(\Omega,\mathcal F)$. By the Kolmogorov consistency theorem, there exists a unique probability measure $\mathbb{P}$ on the product space $(\Omega^{\mathbb N_0},\mathcal F^{\otimes\mathbb N_0})$ such that for the coordinate process $\{X_k\}$,<br>$$<br>\mathbb P(X_0\in A_0,\dots,X_n\in A_n)<br>=<br>\int_{A_0}\mu_0(dx_0)\int_{A_1}P(x_0,dx_1)\cdots\int_{A_n}P(x_{n-1},dx_n),<br>\qquad A_0,\dots,A_n\in\mathcal F.<br>$$</p>
<p>Equivalently:</p>
<ul>
<li><p>$X_0\sim\mu_0$;</p>
</li>
<li><p>for every $k\ge 0$ and $A\in\mathcal F$,</p>
</li>
</ul>
<p>$$<br>\mathbb P(X_{k+1}\in A\mid X_0,\dots,X_k)<br>=<br>\mathbb P(X_{k+1}\in A\mid X_k)<br>\quad \text{a.s.} \qquad \text{(Markov property)}.<br>$$</p>
<p>In this case, the coordinate process $(X_k)_{k\ge 0}$ (and, more generally, any process with the same finite-dimensional<br>distributions) is called a Markov chain with initial distribution $\mu_0$ and transition kernel $P$.</p>
<h3 id="Invariant-Measure"><a href="#Invariant-Measure" class="headerlink" title="Invariant Measure"></a>Invariant Measure</h3><p><strong>Definition:</strong> A probability measure $\pi$ on $(\Omega,\mathcal F)$ is <strong>invariant (stationary)</strong> for $P$ if<br>$$<br>(\pi P)(A):=\int_\Omega \pi(dx)\ P(x,A) =\pi(A).<br>$$<br>Equivalently, if $X_0\sim\pi$, then $X_k\sim\pi$ for every $k\ge 1.$</p>
<h3 id="Markov-Operator"><a href="#Markov-Operator" class="headerlink" title="Markov Operator"></a>Markov Operator</h3><p>Consider the Hilbert space $L^2=L^2(\Omega,\mathcal{F},\pi)$ with the inner product<br>$$<br>\langle f,g\rangle_\pi:=\int_\Omega f\ g\  d\pi.<br>$$<br>Every Markov scheme defines a positive linear operator $M:L^2\to L^2$ by<br>$$<br>(Mf)(x) := \int_\Omega f(y)\ P(x, dy).<br>$$</p>
<h2 id="Reversibility"><a href="#Reversibility" class="headerlink" title="Reversibility"></a>Reversibility</h2><p>Roughly speaking, a Markov scheme is reversible if for any two sets $A,B\in \mathcal{F}$, it steps from $A$ to $B$ as often as from $B$ to $A$.</p>
<p><strong>Definition:</strong> A Markov scheme is reversible with respect to probability measure $\pi$ if<br>$$<br>\pi(dx)P(x,dy) = \pi(dy)P(y,dx),\quad x,y\in \Omega<br>$$<br><strong>Lemma 1:</strong> The followings are equivalent:</p>
<p>(1) Markov scheme is reversible with respect to probability measure $\pi$;</p>
<p>(2) for any two sets $A,B\in \mathcal{F}$,<br>$$<br>\int_B \pi(dx)P(x,A) = \int_A\pi(dx)P(x,B);<br>$$<br>(3) for any measurable function $F:\Omega\times \Omega \to\mathbb{R}$ that makes the integral below finite, we have<br>$$<br>\int_\Omega\int_\Omega F(x,y)\ \pi(dx)P(x,dy) = \int_\Omega\int_\Omega F(y,x)\ \pi(dx)P(x,dy);<br>$$<br>(4) the Markov operator $M$ is self-adjoint.</p>
<p><strong>Proof:</strong> $(1)\Longrightarrow (2):$  for any two sets $A,B\in \mathcal{F}$,<br>$$<br>\int_B \pi(dx)P(x,A)  = \int_B \pi(dx)\int_A P(x,dy)\xlongequal{\text{Fubini}} \int_A\int_B \pi(dx)P(x,dy)\xlongequal{(1)} \int_A\int_B \pi(dy)P(y,dx)\xlongequal{\text{Fubini}}\int_A\pi(dy)\int_B P(y,dx)=\int_A\pi(dy)P(y,B)= \int_A\pi(dx)P(x,B);<br>$$<br>$(2)\Longrightarrow (3):$ it is easy to show that (3) holds for indicator functions $F(x,y)=1_A(x)1_B(y)$ and then (3) holds for any bounded measurable function $F:\Omega\times \Omega \to\mathbb{R}$ by just using the Monotone Class Theorem.</p>
<p>$(3)\Longrightarrow (4):$ For $f,g$  in $L^2$,<br>$$<br>\langle f,Mg\rangle_\pi=\int_\Omega f(x)\ (Mg)(x)\ \pi(dx) = \int_\Omega f(x)\int_\Omega g(y)\ P(x, dy)\ \pi(dx) \xlongequal{\text{Fubini}}\int_\Omega\int_\Omega f(x)g(y)\ \pi(dx)P(x,dy)<br>$$</p>
<p>$$<br>\langle Mf,g\rangle_\pi= \int_\Omega (Mf)(x)\ g(x)\  \pi(dx)= \int_\Omega g(x)\int_\Omega f(y)\ P(x, dy)\ \pi(dx)\xlongequal{\text{Fubini}}\int_\Omega\int_\Omega f(y)g(x)\ \pi(dx)P(x,dy)<br>$$</p>
<p>By using (3) with $F(x,y)=f(x)g(y)$, we obtain $\langle f,Mg\rangle_\pi=\langle Mf,g\rangle_\pi$ directly.</p>
<p>$(4)\Longrightarrow (1):$ Let $f=1_A$ and $g=1_B$, the desired result follows directly by the above equations.</p>
<p><strong>Lemma 2:</strong> If the Markov scheme is reversible with respect to probability measure $\pi$, then for any $f\in L^2$,<br>$$<br>|\langle f,Mf\rangle_\pi|\le \langle f,f\rangle_\pi,<br>$$<br>and the equation holds when $f$ is a constant function. Thus the spectral radius of $M$ is exactly $1$.</p>
<p><strong>Proof:</strong> For any $f\in L^2$, applying Lemma 1(3) with $F(x,y)=f(x)$, it is easy to show that<br>$$<br>\langle f,f\rangle_\pi\ \pm\ \langle f,Mf\rangle_\pi=\frac{1}{2}\int_\Omega\int_\Omega (f(x)\pm f(y))^2\ \pi(dy)P(y,dx)\ge 0.<br>$$<br>And then the desired result follows.</p>
<p><strong>Lemma 3:</strong> If the Markov scheme is reversible with respect to probability measure $\pi$, then $\pi$ is an invariant measure of the Markov scheme.</p>
<p><strong>Proof:</strong> For all $A\in\mathcal{F}$,<br>$$<br>(\pi P)(A)=\int_\Omega \pi(dx)\ P(x,A)=\int_\Omega \pi(dx)\ \int_A P(x,dy)\xlongequal{\text{Fubini}}\int_\Omega \int_\Omega 1_A(y)\pi(dx)P(x,dy)\xlongequal{\text{Lemma 1(3)}}\int_\Omega \int_\Omega 1_A(x)\pi(dx)P(x,dy)=\pi(A).<br>$$</p>
<h2 id="Laziness-of-Markov-Chains"><a href="#Laziness-of-Markov-Chains" class="headerlink" title="Laziness of Markov Chains"></a>Laziness of Markov Chains</h2><p><strong>Definition:</strong> We call the Markov scheme <strong>lazy</strong> if<br>$$<br>P(x,\{x\})\ge \frac12\qquad\text{for all }x\in\Omega.<br>$$<br>(Informally: with probability at least $1/2$ the chain stays put.)</p>
<p><strong>Lemma 4:</strong> If the Markov scheme is lazy and reversible, then $M$ is positive semi-definite.</p>
<p><strong>Proof:</strong>  Consider the operator $2M-I$, which corresponds to the kernel<br>$$<br>\widetilde P(x,A):=2P(x,A)-\mathbf 1_A(x).<br>$$<br>Laziness ensures $\widetilde P(x,\cdot)$ is still a probability measure (nonnegativity at $\{x\}$ is exactly $2P(x,\{x\})-1\ge 0$), hence $2M-I$ is again a Markov operator and is self-adjoint. And then by Lemma 2,<br>$$<br>\langle f,Mf\rangle_\pi<br>=<br>\frac12\langle f,f\rangle_\pi<br>+<br>\frac12\langle f,(2M-I)f\rangle_\pi<br>\ge \frac12\langle f,f\rangle_\pi-\frac12\langle f,f\rangle_\pi=0.<br>$$<br>And then the desired result follows.</p>
<p><strong>Note:</strong> Every Markov scheme can be made lazy by simply tossing a coin at each step and making a move only if it is tails. So (at the cost of a little slow-down) we can assume that $M$ is positive semi-definite, which will be very convenient. </p>
<h2 id="Ergodic-Flow"><a href="#Ergodic-Flow" class="headerlink" title="Ergodic Flow"></a>Ergodic Flow</h2><p><strong>Definition: ** We define the **ergodic flow</strong> $\Phi: \mathcal{F}\to[0,1]$ of the Markov scheme by<br>$$<br>\Phi(A) = \int_A P(x,A^c)\pi(dx),<br>$$<br>which is the probability of the event that in choosing $X_0$ from the invariant distribution we have $X_0\in A$ but $X_1\notin A$.</p>
<p><strong>Lemma 5:</strong> For all $A\in \mathcal{F}$, we have $\Phi(A)=\Phi(A^c)$.</p>
<p>*<em>Proof: *</em> For all $A\in \mathcal{F}$, from the assumption that $\pi$ is invariant, we get<br>$$<br>\Phi(A)-\Phi(A^c) = \int_{A} P(x,A^c)\pi(dx)-\int_{A^c} P(x,A)\pi(dx)=\int_{A} (1-P(x,A))\pi(dx)-\int_{A^c} P(x,A)\pi(dx)=\pi(A)-\int_\Omega P(x,A)\pi(dx) = 0.<br>$$<br>And then the desired result follows.</p>
<p>Note that the computation also works backward, we have the following lemma.</p>
<p><strong>Lemma 6:</strong> If $\nu$ is any probability measure on $(\Omega,\mathcal{F})$ such that the set-function<br>$$<br>\Psi(A) =\int_A P(x,A^c)\nu(dx)<br>$$<br>is invariant under complementation, namely, $\Psi(A)=\Psi(A^c)$, then $\nu$ is invariant.</p>
<p>*<em>Proof: *</em> For all $A\in \mathcal{F}$, from the assumption that $\Psi(A)=\Psi(A^c)$, we have<br>$$<br>0=\Psi(A)-\Psi(A^c)=\int_{A} P(x,A^c)\nu(dx)-\int_{A^c} P(x,A)\nu(dx)=\int_{A} (1-P(x,A))\nu(dx)-\int_{A^c} P(x,A)\nu(dx)=\nu(A)-\int_\Omega P(x,A)\nu(dx),<br>$$<br>which means $\nu P=\nu$, $\nu$ is invariant.</p>
<h2 id="Conductance"><a href="#Conductance" class="headerlink" title="Conductance"></a>Conductance</h2><p><strong>Definition: *<em>The *</em>conductance</strong> of the Markov scheme is<br>$$<br>\Phi:=\inf_{0&lt;\pi(A)&lt;1/2} \frac{\Phi(A)}{\pi(A)}.<br>$$<br>For every $0\le s\le 1$, the <strong>$s-$conductance</strong> is defined by<br>$$<br>\Phi_s:=\inf_{s&lt;\pi(A)&lt;1/2} \frac{\Phi(A)}{\pi(A)-s}.<br>$$<br>And we call the value $1-P(x,\{x\})$ the <strong>local conductance</strong> of the Markov scheme at element $x$.</p>
<p><strong>Lemma 7:</strong> If the invariant measure $\pi$ has an atom $y$ (i.e. $\pi(\{y\})&gt;0$), then the local conductance of the Markov scheme at element $y$ is just $\Phi(\{y\})/\pi(\{y\})$, hence<br>$$<br>\Phi\le \frac{\Phi(\{y\})}{\pi(\{y\})} =1-P(y,\{y\}).<br>$$<br>*<em>Proof: *</em><br>$$<br>\Phi(\{y\})= \int_{\{y\}} P(x,\{y\}^c)\pi(dx)=P(\{y\},\{y\}^c)\pi(\{y\})= (1-P(y,\{y\}))\pi(\{y\})<br>$$<br>Since $\pi(\{y\})&gt;0$, we get the desired result.</p>
<p><strong>Lemma 8:</strong> Let<br>$$<br>H_t=\{x\in\Omega<del>;</del> 1-P(x,\{x\})&lt;t\},<br>$$<br>and $s=\pi(H_t)$. Then<br>$$<br>\Phi(H_t)&lt;t\  \pi(H_t).<br>$$<br>As a consequence, if $s&lt;1/2$, then the $(s/2)-$conductance of the Markov scheme is at most $2t$.<br>*<em>Proof: *</em><br>$$<br>\Phi(H_t)=\int_{H_t} P(x,H_t^c)\pi(dx)&lt;\int_{H_t} P(x,\{x\}^c)\pi(dx)&lt;t\  \pi(H_t)=ts.<br>$$<br>If $s&lt;1/2$, then<br>$$<br>\Phi_{s/2}= \inf_{s/2&lt;\pi(A)&lt;1/2} \frac{\Phi(A)}{\pi(A)-s/2}\le \frac{\Phi(H_t)}{\pi(H_t)-s/2}&lt; \frac{ts}{s-s/2}=2t.<br>$$</p>
<h2 id="Mixing-Time"><a href="#Mixing-Time" class="headerlink" title="Mixing Time"></a>Mixing Time</h2><p>Let $\mu_k$ denotes the law of $X_k$. By definition, we have the recurrence<br>$$<br>\mu_k(A)=\int_\Omega \mu_{k-1}(dx)P(x,A)<br>$$<br>The reason for us to introduce the definition of conductance is that if the conductance $\Phi&gt;0$, then<br>$$<br>\operatorname{TV}(\mu_k,\pi):=\sup_{A\in\mathcal{F}}|\mu_k(A)-\pi(A)|\to 0 \quad \text{as  }k\to \infty.<br>$$<br>We have the following theoerms.</p>
<p><strong>Theorem 1:</strong> Let<br>$$<br>M=\sup_{A\in \mathcal{F}} \frac{\mu_0(A)}{\pi(A)}\tag{warm-start}.<br>$$<br>Then<br>$$<br>\operatorname{TV}(\mu_k,\pi):=\sup_{A\in\mathcal{F}}|\mu_k(A)-\pi(A)|\le \sqrt{M}\left(1-\frac{1}{2}\Phi^2\right)^k<br>$$<br><strong>Theorem 2:</strong> Let $0&lt;s\le 1/2$ and $H_s=\sup\{|\mu_0(A)-\pi(A)|<del>;</del> \pi(A)\le s\}$. Assume that each atom has $\pi-$measure less than $1/2$, then<br>$$<br>\operatorname{TV}(\mu_k,\pi):=\sup_{A\in\mathcal{F}}|\mu_k(A)-\pi(A)|\le 2H_s+\frac{2H_s}{s}\left(1-\frac{1}{2}\Phi_s^2\right)^k<br>$$<br>Furthermore, if $\pi$ is atom-free, then<br>$$<br>\operatorname{TV}(\mu_k,\pi):=\sup_{A\in\mathcal{F}}|\mu_k(A)-\pi(A)|\le H_s+\frac{H_s}{s}\left(1-\frac{1}{2}\Phi_s^2\right)^k<br>$$</p>
<p>See [1] for the proofs of Theorems 1 and 2.</p>
<p><strong>Definition:</strong> The mixing time in total variation distance is defined by<br>$$<br>t_{\text{TV}}(\varepsilon,\mu_0):=\inf\{k\in \mathbb{N}<del>|</del> \operatorname{TV}(\mu_k,\pi)\le \varepsilon\}<br>$$</p>
<h2 id="Relation-to-Sampling-Algorithms"><a href="#Relation-to-Sampling-Algorithms" class="headerlink" title="Relation to Sampling Algorithms"></a>Relation to Sampling Algorithms</h2><p><strong>In non-asymptotic analysis of the convergence of sampling algorithms, mixing time plays a central role.</strong></p>
<p>In much of the literature on sampling algorithms, it is common to assume <strong>warm start</strong> for the initial distribution.</p>
<p><strong>Definition: ** We say that the initial distribution $\mu_0$ is **$M-$warm</strong>, if it satisfies<br>$$<br>\sup_{A\in \mathcal{F}} \frac{\mu_0(A)}{\pi(A)}\le M<br>$$<br>Under $M-$warm assumption, and if target distribution is atom-free, then $H_s$ defined in Theorem 2 above satisfies<br>$$<br>H_s=\sup\{|\mu_0(A)-\pi(A)|<del>;</del> \pi(A)\le s\}\le \min\{s,(M-1)s\}\le Ms.<br>$$<br>Then by Theorem 2, we get<br>$$<br>\operatorname{TV}(\mu_k,\pi)\le Ms+ M\left(1-\frac{1}{2}\Phi_s^2\right)^k\le Ms+ Me^{-\frac{n}{2}\Phi_s^2}.<br>$$<br>Then $\operatorname{TV}(\mu_k,\pi)\le \varepsilon$ follows from taking<br>$$<br>s=\frac{\varepsilon}{2M},\quad k\ge \frac{2}{\Phi_s^2}\log\frac{2M}{\varepsilon}.<br>$$<br>Hence, the problem of analyzing the mixing time is reduced to controlling the $s-$conductance $\Phi_s$.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Lovász, László, and Miklós Simonovits. “Random walks in a convex body and an improved volume algorithm.” <em>Random structures &amp; algorithms</em> 4.4 (1993): 359-412.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Reversible, Conductance and Rapid Mixing of Markov Chains</p><p><a href="https://handsteinwang.github.io/2025/12/18/Markov_Chain_1/">https://handsteinwang.github.io/2025/12/18/Markov_Chain_1/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Handstein Wang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2025-12-18</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2025-12-18</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Sampling/">Sampling</a><a class="link-muted mr-2" rel="tag" href="/tags/Stochastic-Process/">Stochastic Process</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2026/02/15/report_2025/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">2025资产配置报告</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2025/11/27/flow_matching/"><span class="level-item">Introduction to Flow Matching</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/quokka.png" alt="Handstein Wang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Handstein Wang</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Australia</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives/"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories/"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags/"><p class="title">7</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://wangtuo2002.github.io/" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://wangtuo2002.github.io/"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Email" href="mailto:wangtuo1020@outlook.com"><i class="fas fa-envelope"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://sep.ucas.ac.cn/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">SEP-UCAS</span></span><span class="level-right"><span class="level-item tag">sep.ucas.ac.cn</span></span></a></li><li><a class="level is-mobile" href="https://online.amss.ac.cn/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Online-AMSS</span></span><span class="level-right"><span class="level-item tag">online.amss.ac.cn</span></span></a></li><li><a class="level is-mobile" href="https://overleaf.com/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Overleaf</span></span><span class="level-right"><span class="level-item tag">overleaf.com</span></span></a></li><li><a class="level is-mobile" href="https://mathtube.org/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">MathTube</span></span><span class="level-right"><span class="level-item tag">mathtube.org</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Investment/"><span class="level-start"><span class="level-item">Investment</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Sampling/"><span class="level-start"><span class="level-item">Sampling</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2026-02-15T00:27:29.670Z">2026-02-15</time></p><p class="title"><a href="/2026/02/15/report_2025/">2025资产配置报告</a></p><p class="categories"><a href="/categories/Investment/">Investment</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-12-17T16:00:00.000Z">2025-12-18</time></p><p class="title"><a href="/2025/12/18/Markov_Chain_1/">Reversible, Conductance and Rapid Mixing of Markov Chains</a></p><p class="categories"><a href="/categories/Sampling/">Sampling</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-11-26T16:00:00.000Z">2025-11-27</time></p><p class="title"><a href="/2025/11/27/flow_matching/">Introduction to Flow Matching</a></p><p class="categories"><a href="/categories/Sampling/">Sampling</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-10-23T16:00:00.000Z">2025-10-24</time></p><p class="title"><a href="/2025/10/24/diffusion/">Introduction to Diffusion Model</a></p><p class="categories"><a href="/categories/Sampling/">Sampling</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-10-19T16:00:00.000Z">2025-10-20</time></p><p class="title"><a href="/2025/10/20/sampling_introduction/">Introduction to Sampling</a></p><p class="categories"><a href="/categories/Sampling/">Sampling</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2026/02/"><span class="level-start"><span class="level-item">February 2026</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/12/"><span class="level-start"><span class="level-item">December 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/11/"><span class="level-start"><span class="level-item">November 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/10/"><span class="level-start"><span class="level-item">October 2025</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/09/"><span class="level-start"><span class="level-item">September 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Diffusion-Model/"><span class="tag">Diffusion Model</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Generative-Modeling/"><span class="tag">Generative Modeling</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Optimization/"><span class="tag">Optimization</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Paper-Reading/"><span class="tag">Paper Reading</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Report/"><span class="tag">Report</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sampling/"><span class="tag">Sampling</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Stochastic-Process/"><span class="tag">Stochastic Process</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/hw.svg" alt="Handstein Wang" height="28"></a><p class="is-size-7"><span>&copy; 2026 Handstein Wang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>